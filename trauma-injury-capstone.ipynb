{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"The project aims to address the critical issue of prompt and accurate diagnosis of abdominal trauma, which is a common cause of death and a major public health concern globally. Abdominal trauma, often resulting from motor vehicle accidents, can lead to severe injuries to internal organs and internal bleeding### **Background Information:**\n\nTraumatic injury is a significant global health concern, especially affecting individuals in the first four decades of life. It is responsible for millions of annual deaths worldwide and poses a substantial public health challenge. Prompt and accurate diagnosis of traumatic injuries is crucial for improving patient outcomes and increasing survival rates. Among various diagnostic tools, computed tomography (CT) has emerged as a vital technology for evaluating individuals suspected of having abdominal injuries. CT scans provide detailed cross-sectional images of the abdomen, aiding in the detection and assessment of traumatic injuries.\n\n\nInterpreting CT scans for abdominal trauma can be a complex and time-consuming task, particularly when dealing with multiple injuries or subtle areas of active bleeding. This complexity often requires the expertise of medical professionals, and even for them, it can be challenging to make rapid and precise diagnoses. The need for timely intervention and appropriate treatment underscores the importance of improving the diagnostic process.\n\n\n\n### **Problem Statement:**\n\nWith more than 5 million deaths caused by traumatic injury each year, it is the largest cause of early-life mortality and a major public health concern worldwide. Among these, blunt abdominal trauma is frequently sustained in car accidents and can cause serious internal bleeding and damage. In Kenya, a country of over 50 million people, this challenge is magnified by the severe shortage of healthcare infrastructureâ€”only about 50 CT scanners and 200 trained radiologists are available nationwide. This shortage leads to misdiagnoses, delayed treatments due to average waiting times of several weeks, and a lack of access to vital healthcare services for many Kenyans. Despite government initiatives to invest in new CT scanners and train more radiologists, the need for rapid and accurate diagnosis remains critical. However, it is sometimes difficult and time-consuming for medical personnel to interpret CT scans for abdominal injuries. Therefore, there is an urgent need for automated, accurate, and rapid diagnostic solutions as any delay can be fatal.\n\n\n\n### **Objectives:**\n\n* To develop AI algorithms that can automatically and accurately detect traumatic injuries to internal abdominal organs using CT scans.\n\n* To classify the discovered injuries according to their severity, thereby providing medical experts a vital tool to start proper treatment.\n\n* To rigorously evaluate the developed algorithms using performance metrics that are relevant for both machine learning models and clinical applicability.\n\n\n\n\n\n### **Research Questions:**\n\n* How effective are AI algorithms in automatically detecting traumatic injuries to internal abdominal organs like the liver, kidneys, spleen, and bowel using CT scans?\n\n* What features and patterns in CT scans are most indicative of different severities of abdominal injuries, and how can they be utilized for automated injury grading?\n\n* What are the appropriate metrics for evaluating the performance of the developed AI algorithms in terms of both machine learning benchmarks and clinical utility?","metadata":{"_uuid":"f3a5fbbc-7fc5-439e-89ae-75a0ec8d9461","_cell_guid":"67f48caa-be20-4466-88d0-b66425dca0d1","_kg_hide-input":false,"_kg_hide-output":false,"trusted":true}},{"cell_type":"markdown","source":"### **Importing Libraries**","metadata":{"_uuid":"306118fe-088e-454f-943b-e0ebccc06e38","_cell_guid":"298ae7c8-47e7-4a0f-90e6-48c3bf245757","trusted":true}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport pydicom\nimport matplotlib.pyplot as plt\nimport cv2\nimport seaborn as sns\nimport tensorflow as tf\nimport os","metadata":{"_uuid":"0fd7ed6c-6564-4023-84bd-bbcc483783bb","_cell_guid":"cf7ad05f-7572-4f5b-9d2a-5104b8efaeab","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:31:35.754973Z","iopub.execute_input":"2023-10-02T16:31:35.755495Z","iopub.status.idle":"2023-10-02T16:31:35.760535Z","shell.execute_reply.started":"2023-10-02T16:31:35.755463Z","shell.execute_reply":"2023-10-02T16:31:35.759096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Loading the datasets**","metadata":{"_uuid":"e7bfb316-0aa7-419f-97dd-5802f7094bbb","_cell_guid":"bbde70d8-ef5b-43a2-a3cb-209479b7ef94","trusted":true}},{"cell_type":"code","source":"labels = pd.read_csv('/kaggle/input/rsna-2023-abdominal-trauma-detection/image_level_labels.csv')\ntrain=pd.read_csv('/kaggle/input/rsna-2023-abdominal-trauma-detection/train.csv')\ntrain_meta = pd.read_csv('/kaggle/input/rsna-2023-abdominal-trauma-detection/train_series_meta.csv')\ntest_meta = pd.read_csv('/kaggle/input/rsna-2023-abdominal-trauma-detection/test_series_meta.csv')","metadata":{"_uuid":"5cdbdc25-b924-42bb-8118-e95cef92c03c","_cell_guid":"c1595fa7-c344-4cfd-b8bb-85ce8d2b8d9a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:31:35.762998Z","iopub.execute_input":"2023-10-02T16:31:35.763862Z","iopub.status.idle":"2023-10-02T16:31:35.795465Z","shell.execute_reply.started":"2023-10-02T16:31:35.763827Z","shell.execute_reply":"2023-10-02T16:31:35.794547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Displaying the first few rows of each dataset\ntrain.head(), labels.head(), train_meta.head()","metadata":{"_uuid":"13ada4c4-e572-483b-98df-c7e335fc7168","_cell_guid":"901fb893-75dc-4a73-a8f4-cf3aff7456df","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:31:35.797558Z","iopub.execute_input":"2023-10-02T16:31:35.797903Z","iopub.status.idle":"2023-10-02T16:31:35.812933Z","shell.execute_reply.started":"2023-10-02T16:31:35.797869Z","shell.execute_reply":"2023-10-02T16:31:35.811651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**labels (label.csv):**\n\n* patient_id: The unique identifier for each patient.\n* series_id: Identifier for the series of images for the patient.\n* instance_number: The specific image instance number within the series.\n* injury_name: The name or type of injury detected in the image.\n\n**train (train.csv):**\n\nThis dataset provides the labels for different types of injuries for each patient.\nColumns like bowel_healthy, bowel_injury, extravasation_healthy, etc., indicate the health status or injury severity of various organs for each patient.\n\n**train_meta (train_series_meta.csv):**\n\n* patient_id: The unique identifier for each patient.\n* series_id: Identifier for the series of images for the patient.\n* aortic_hu: A quantitative measure related to the images.\n* incomplete_organ: A binary indicator specifying whether the organ is incomplete in the images.","metadata":{"_uuid":"29132157-788d-496d-9a57-17c3ba593e2c","_cell_guid":"dd105b85-e1af-420f-96bb-a7cbc54bdce8","trusted":true}},{"cell_type":"code","source":"merged_df = pd.merge(train, train_meta, on='patient_id', how='inner')","metadata":{"_uuid":"5b776336-8f97-4545-a968-500b7d503a45","_cell_guid":"cbdf0b2c-9ffd-47f7-b30a-75b689aa81e1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:31:35.814154Z","iopub.execute_input":"2023-10-02T16:31:35.815050Z","iopub.status.idle":"2023-10-02T16:31:35.827321Z","shell.execute_reply.started":"2023-10-02T16:31:35.815018Z","shell.execute_reply":"2023-10-02T16:31:35.826406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"complete_df = pd.merge(merged_df, labels, on='patient_id', how='inner')\ncomplete_df","metadata":{"_uuid":"308a0fd1-109c-45eb-9ff7-b67862198ae6","_cell_guid":"4c05bad8-2724-4cdd-9040-bb9ba3d4e4a3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:31:35.830097Z","iopub.execute_input":"2023-10-02T16:31:35.830869Z","iopub.status.idle":"2023-10-02T16:31:35.859062Z","shell.execute_reply.started":"2023-10-02T16:31:35.830834Z","shell.execute_reply":"2023-10-02T16:31:35.858083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_df = complete_df.drop(['patient_id', 'any_injury','series_id_x','series_id_y', 'instance_number', 'injury_name'], axis=1)","metadata":{"_uuid":"b66c8e8a-8c23-4c96-ab5d-dea79ccb2b27","_cell_guid":"fc7f1a1d-d254-4882-b045-d6d2af345377","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:31:35.860528Z","iopub.execute_input":"2023-10-02T16:31:35.861102Z","iopub.status.idle":"2023-10-02T16:31:35.867642Z","shell.execute_reply.started":"2023-10-02T16:31:35.861065Z","shell.execute_reply":"2023-10-02T16:31:35.866703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlation_matrix = corr_df.corr()\ncorrelation_matrix","metadata":{"_uuid":"4d01b640-5c18-4a70-8f24-fb3e9968834b","_cell_guid":"4b83704f-41b6-4983-b694-5ebfe4cd102c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:31:35.869091Z","iopub.execute_input":"2023-10-02T16:31:35.869810Z","iopub.status.idle":"2023-10-02T16:31:35.916127Z","shell.execute_reply.started":"2023-10-02T16:31:35.869776Z","shell.execute_reply":"2023-10-02T16:31:35.915034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nplt.figure(figsize=(10, 8))\n# Create the heatmap\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n\n# Add a title\nplt.title('Correlation Matrix Heatmap')\n\n# Show the plot\nplt.show()","metadata":{"_uuid":"86230d05-2bf2-4318-8f02-cd4535c8ed99","_cell_guid":"687b21f1-0a9a-422b-8c16-5ea873923e54","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:31:35.917663Z","iopub.execute_input":"2023-10-02T16:31:35.918295Z","iopub.status.idle":"2023-10-02T16:31:36.709196Z","shell.execute_reply.started":"2023-10-02T16:31:35.918239Z","shell.execute_reply":"2023-10-02T16:31:36.708281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlation_with_aortic_hu = corr_df.corr()['aortic_hu']\nplt.figure(figsize=(12, 6))\ncorr_df.corr()['aortic_hu'].plot(kind='bar', color='skyblue')\nplt.xlabel('Columns')\nplt.ylabel('Correlation')\nplt.title('Correlation of \"aortic_hu\" with Other Columns')\nplt.xticks(rotation=90)\nplt.show()","metadata":{"_uuid":"810feb96-5596-416c-9499-f69b780c2965","_cell_guid":"0c39555c-0d38-4821-baf3-5729e1b04889","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:31:36.710733Z","iopub.execute_input":"2023-10-02T16:31:36.711344Z","iopub.status.idle":"2023-10-02T16:31:37.040625Z","shell.execute_reply.started":"2023-10-02T16:31:36.711311Z","shell.execute_reply":"2023-10-02T16:31:37.039664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Understanding**","metadata":{"_uuid":"b14ae972-4dcf-4360-b1f8-cf388ac8f5ca","_cell_guid":"509a5a72-2e57-408f-b69a-bac1805ab5e5","trusted":true}},{"cell_type":"code","source":"# Basic information for the 'train' dataset\ntrain_info = {\n    \"Number of Rows\": train.shape[0],\n    \"Number of Columns\": train.shape[1],\n    \"Columns\": train.columns.tolist(),\n    \"Data Types\": train.dtypes.tolist(),\n    \"Unique Values per Column\": train.nunique().tolist()\n}\n\n# Basic information for the 'labels' dataset\nlabels_info = {\n    \"Number of Rows\": labels.shape[0],\n    \"Number of Columns\": labels.shape[1],\n    \"Columns\": labels.columns.tolist(),\n    \"Data Types\": labels.dtypes.tolist(),\n    \"Unique Values per Column\": labels.nunique().tolist()\n}\n\n# Basic information for the 'train_meta' dataset\ntrain_meta_info = {\n    \"Number of Rows\": train_meta.shape[0],\n    \"Number of Columns\": train_meta.shape[1],\n    \"Columns\": train_meta.columns.tolist(),\n    \"Data Types\": train_meta.dtypes.tolist(),\n    \"Unique Values per Column\": train_meta.nunique().tolist()\n}\n\ntrain_info, labels_info, train_meta_info","metadata":{"_uuid":"423638c0-8943-4cc1-a917-b70608e2a267","_cell_guid":"20823d64-b7f1-4026-9ff1-8b8a04ea3f2b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:31:37.043841Z","iopub.execute_input":"2023-10-02T16:31:37.044809Z","iopub.status.idle":"2023-10-02T16:31:37.063428Z","shell.execute_reply.started":"2023-10-02T16:31:37.044764Z","shell.execute_reply":"2023-10-02T16:31:37.062252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**1. labels (image_level_labels.csv) Dataset:**\n\n- Number of Rows: 12,029\n- Number of Columns: 4\n- Columns:\n    - patient_id: Unique dentifier of the patient.\n    - series_id: An identifier for the series of images associated with each patient..\n    - instance_number: Specific image instance number within the series.\n    - injury_name: Type of injury detected in the image.\n- Data Types: The data types are appropriate with integer types for identifiers and object (string) type for the injury name.\n- Unique Values: There are 246 unique patients, 330 unique series, and 925 unique instance numbers. The injury_name column has 2 unique values, indicating two types of injuries;  Active_Extravasation and bowel\n\n**2. train(train.csv) Dataset:**\n\n- Number of Rows: 3,147\n- Number of Columns: 15\n- Columns:\n    - patient_id: Unique identifier of the patient.\n    - The other 14 columns represent the health status and injury severity of various organs for each patient, recorded as binary variables where 0 indicates the absence of a condition, and 1 indicates the presence of a condition.\n- Data Types: All columns are of integer type.\n- Unique Values: There are 3,147 unique patients. The injury-related columns have binary values (0 or 1), indicating the absence or presence of a specific injury type.\n\n**3. train_meta (train_series_meta.csv) Dataset:**\n\n- Number of Rows: 4,711\n- Number of Columns: 4\n- Columns:\n    - patient_id: Unique identifier of the patient.\n    - series_id: An identifier for the series of images associated with each patient..\n    - aortic_hu: A quantitative measure in HU related to the aorta.\n    - incomplete_organ: A binary indicator where 0 signifies the absence of an incomplete organ, and 1 signifies the presence of an incomplete organ..\n- Data Types: The data types are appropriate with integer and float types.\n- Unique Values: There are 3,147 unique patients and 4,711 unique series. The incomplete_organ column has 2 unique values.","metadata":{"_uuid":"fc2c0bf7-362a-4e02-8b76-de1f3f81b390","_cell_guid":"6f1bf329-681f-4fd3-ab26-0eab08285619","trusted":true}},{"cell_type":"code","source":"# Checking for missing values and duplicates\ndef check_missing_and_duplicates(datasets):\n    # Initializing lists to store the results\n    dataset_names = []\n    missing_values_list = []\n    duplicates_list = []\n    \n    for dataset_name, dataset in datasets.items():\n        # Calculating missing values\n        missing_values = dataset.isnull().sum().sum()\n        \n        # Checking for duplicates\n        duplicates = dataset.duplicated().sum()\n        \n        # Appending \n        dataset_names.append(dataset_name)\n        missing_values_list.append(missing_values)\n        duplicates_list.append(duplicates)\n    \n    # Creating a summary DataFrame\n    summary_df = pd.DataFrame({\n        \"Dataset\": dataset_names,\n        \"Missing Values\": missing_values_list,\n        \"Duplicates\": duplicates_list\n    })\n    \n    return summary_df\n\ndatasets = {\n    \"train\": train,\n    \"labels\": labels,\n    \"train_meta\": train_meta\n}\n\nsummary = check_missing_and_duplicates(datasets)\n\nprint(summary)","metadata":{"_uuid":"031e63b1-2254-4b60-9a4c-0036936ccd1e","_cell_guid":"5966d27f-df6f-4251-a92d-5d5b9803378b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:31:37.064761Z","iopub.execute_input":"2023-10-02T16:31:37.065809Z","iopub.status.idle":"2023-10-02T16:31:37.085771Z","shell.execute_reply.started":"2023-10-02T16:31:37.065763Z","shell.execute_reply":"2023-10-02T16:31:37.084640Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- There are no missing values in any of the datasets.\n- There are no duplicated rows in any of the datasets.","metadata":{"_uuid":"e79d58e2-beef-47a0-b3dd-250cb051f040","_cell_guid":"359e9707-730c-4886-81cc-f204415a53d1","trusted":true}},{"cell_type":"code","source":"print(\"Descriptive statistics for the train dataset:\")\nprint(train.describe())","metadata":{"_uuid":"82210c83-cb56-4509-bfbc-65ae7f233212","_cell_guid":"caa0fe9c-1f78-4bee-afaf-b31370352461","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:31:37.087508Z","iopub.execute_input":"2023-10-02T16:31:37.088196Z","iopub.status.idle":"2023-10-02T16:31:37.134556Z","shell.execute_reply.started":"2023-10-02T16:31:37.088160Z","shell.execute_reply":"2023-10-02T16:31:37.133437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Statistical Summary of the train dataset**\n\n**1. Patient IDs Distribution:**\n\n- Representing the unique patient identifier, the 'patient_id' column ranges from 19 to 65,508, suggesting a wide range of patients in the dataset.\n\n**2. Organ Health Status:**\n\n- Several columns (e.g., 'bowel_healthy', 'extravasation_healthy', 'kidney_healthy', 'liver_healthy', 'spleen_healthy') are binary indicators of organ health.\n- On average, most patients have healthy organs, as indicated by values close to 1.\n- The mean values of these columns are as per below:\n    - The 'bowel_healthy' has an approximate mean of 0.98.\n    - The 'extravasation_healthy' has a 0.94 mean.\n    - The 'kidney_healthy' has 0.94 mean.\n    - The 'liver_healthy'has a 0.90 mean.\n    - The 'spleen_healthy' has a 0.89 mean.\n- This suggests that these organ related injuries are relatively rare in the dataset.\n\n**3. Organ Injury Severity:**\n\n- Columns like 'bowel_injury', 'extravasation_injury', 'kidney_low', 'kidney_high', 'liver_low', 'liver_high', 'spleen_low', and 'spleen_high' represent binary indicators of injury severity for various organs.  The mean values of these columns are as per below:\n    - The 'liver_high'has a mean of around 0.02 while 'liver_low' has a mean of 0.08. \n    - The 'spleen_high' is at 0.05 with 'spleen_low' at 0.06.\n    - The 'kidney_high' is at 0.02 with 'kidney_low' at 0.04.\n    - The 'bowel_injury' is at 0.02 with the 'extravasation_injury' at 0.06.\n- These columns have low mean values, further confirming that severe injuries are relatively uncommon as compared to healthy organs.\n\n**4. Overall Injury Presence:**\n\n- The 'any_injury' column is a binary indicator of the presence of any injury in a patient.\n- On average, approximately 27% of patients in the dataset have at least one injury (mean value of 0.27).\n\n**Conclusions:**\n\n- The dataset appears to be relatively imbalanced, with most patients having healthy organs and a minority experiencing injuries.\n\n- Bowel injuries, extravasation injuries, kidney injuries, liver injuries, and spleen injuries are relatively rare, as indicated by low mean values for their respective columns.\n\n- Most patients have healthy organs, suggesting that the dataset may contain a majority of cases without severe injuries.\n\n**Approximately 27% of patients in the dataset have at least one injury, indicating that injuries, while less common, are still present in a significant portion of the population. This underscores the importance of conducting further EDA to gain deeper insights into the nature, patterns, and potential risk factors associated with these injuries. EDA will help us better understand the characteristics of injuries and their impact on patient outcomes, leading to more informed decision-making in the field of trauma care and intervention.**","metadata":{"_uuid":"bea73e1c-cc42-4960-9154-d561d9dfd3ee","_cell_guid":"af3924ea-b932-48d4-82fc-fa3bf94e9a74","trusted":true}},{"cell_type":"markdown","source":"## **Exploratory data Analysis**\n\n### **Univariate Analysis**","metadata":{"_uuid":"f7b4b572-2dbd-4536-ab81-6cec0ebb1af8","_cell_guid":"c9983fdb-5d37-453e-a053-40e7d88bd9ba","execution":{"iopub.status.busy":"2023-09-21T11:20:23.208756Z","iopub.execute_input":"2023-09-21T11:20:23.209230Z","iopub.status.idle":"2023-09-21T11:20:23.217595Z","shell.execute_reply.started":"2023-09-21T11:20:23.209194Z","shell.execute_reply":"2023-09-21T11:20:23.215839Z"},"trusted":true}},{"cell_type":"code","source":"# Visualizing the distribution of injury types in the 'label' dataset\nplt.figure(figsize=(10, 6))\nsns.countplot(data=labels, x='injury_name')\nplt.title('Distribution of Injury Types in train Dataset')\nplt.ylabel('Count')\nplt.xlabel('Injury Type')\nplt.show()","metadata":{"_uuid":"a8163582-d5db-4150-949e-1e0efd3137a4","_cell_guid":"cf33ca40-5ddb-4b27-92f7-b15029372199","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:31:37.136226Z","iopub.execute_input":"2023-10-02T16:31:37.137198Z","iopub.status.idle":"2023-10-02T16:31:37.377716Z","shell.execute_reply.started":"2023-10-02T16:31:37.137163Z","shell.execute_reply":"2023-10-02T16:31:37.376642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data suggests that extravasation (active bleeding) is more frequently identified in the provided images than bowel injuries.","metadata":{"_uuid":"ec58cc70-578b-483d-994c-bd2787b75003","_cell_guid":"46a5d1d1-c479-4a1e-a13f-59f4946a8e4a","trusted":true}},{"cell_type":"code","source":"# Visualizing the distribution of injury-related columns in the 'train' dataset\ninjury_columns = [col for col in train.columns if col != \"patient_id\"]\ninjury_counts = train[injury_columns].sum()\n\nplt.figure(figsize=(14, 8))\ninjury_counts.sort_values().plot(kind='barh')\nplt.title('Distribution of Injury-Related Columns in labels Dataset')\nplt.xlabel('Count')\nplt.ylabel('Injury Type / Health Status')\nplt.show()","metadata":{"_uuid":"9fb61afc-7d74-4a14-b614-edf0c41ec722","_cell_guid":"c3306b76-b282-4395-b5a8-5bbca3afe0f9","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:31:37.379192Z","iopub.execute_input":"2023-10-02T16:31:37.379801Z","iopub.status.idle":"2023-10-02T16:31:37.707310Z","shell.execute_reply.started":"2023-10-02T16:31:37.379764Z","shell.execute_reply":"2023-10-02T16:31:37.706431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing the distribution of the 'aortic_hu' column in the 'train_meta' dataset\nplt.figure(figsize=(10, 6))\nsns.histplot(train_meta['aortic_hu'], bins=50, kde=True)\nplt.title('Distribution of Aortic HU in train_meta Dataset')\nplt.xlabel('Aortic HU')\nplt.ylabel('Count')\nplt.show()","metadata":{"_uuid":"d8d80b50-d12e-4bfb-8847-9ea34b1a627b","_cell_guid":"aeff8548-3d5b-4580-82f6-9bc86cb8614f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:31:37.708916Z","iopub.execute_input":"2023-10-02T16:31:37.709580Z","iopub.status.idle":"2023-10-02T16:31:38.059294Z","shell.execute_reply.started":"2023-10-02T16:31:37.709545Z","shell.execute_reply":"2023-10-02T16:31:38.058218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"_uuid":"5625bc55-92f9-4c9f-98ad-69a41b686769","_cell_guid":"9bbf423a-5aa2-4861-b2f9-2184913f3e85","trusted":true}},{"cell_type":"markdown","source":"Hounsfield Units (HU) are a measure used in CT scans to describe radiodensity, and the distribution gives us an idea of the variation in these values across different images.","metadata":{"_uuid":"2611a0de-893d-40a4-8ae3-4a2067879d8e","_cell_guid":"403909d2-ae39-4283-b50b-f4c02cc31f25","trusted":true}},{"cell_type":"markdown","source":"**Relationship Analysis:**","metadata":{"_uuid":"ef152e16-45c1-485f-bffd-eaa05f76611e","_cell_guid":"6435bd4b-92a2-4a01-8b15-5745a0df1bf8","trusted":true}},{"cell_type":"code","source":"# Visualizing the relationship between 'aortic_hu' and 'incomplete_organ' in the 'train_meta' dataset\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=train_meta, x='incomplete_organ', y='aortic_hu')\nplt.title('Relationship between Aortic HU and Incomplete Organ in train_meta Dataset')\nplt.xlabel('Incomplete Organ (0 = Complete, 1 = Incomplete)')\nplt.ylabel('Aortic HU')\nplt.show()","metadata":{"_uuid":"1f11beee-6756-4b4c-82e7-c06b2616ed0b","_cell_guid":"1fd537ed-25e3-41da-afbc-447886437183","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:31:38.060622Z","iopub.execute_input":"2023-10-02T16:31:38.061747Z","iopub.status.idle":"2023-10-02T16:31:38.295104Z","shell.execute_reply.started":"2023-10-02T16:31:38.061708Z","shell.execute_reply":"2023-10-02T16:31:38.294162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This suggests that there might be some relationship between the completeness of the organ in the image and the aortic_hu values.","metadata":{"_uuid":"fc0726fc-8b46-4b5f-9e8d-3198a86f4ab9","_cell_guid":"e987beaf-ee5e-41fe-9199-137c85341de9","trusted":true}},{"cell_type":"markdown","source":"**Outliers Analysis:**","metadata":{"_uuid":"15494f72-cac2-44da-aa55-f830bbe78a0c","_cell_guid":"c9336423-97f6-491c-8fd8-e41be76d8380","trusted":true}},{"cell_type":"code","source":"# Outlier analysis for the 'aortic_hu' column using the IQR method\n\n# Calculate Q1, Q3, and IQR\nQ1 = train_meta['aortic_hu'].quantile(0.25)\nQ3 = train_meta['aortic_hu'].quantile(0.75)\nIQR = Q3 - Q1\n\n# Define bounds for outliers\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\n# Identify outliers\noutliers = train_meta[(train_meta['aortic_hu'] < lower_bound) | (train_meta['aortic_hu'] > upper_bound)]\n\n# Percentage of data points that are outliers\noutlier_percentage = (len(outliers) / len(train_meta)) * 100\n\noutlier_summary = {\n    \"Lower Bound\": lower_bound,\n    \"Upper Bound\": upper_bound,\n    \"Number of Outliers\": len(outliers),\n    \"Percentage of Outliers\": outlier_percentage\n}\n\noutlier_summary","metadata":{"_uuid":"120023d5-69c0-4301-b744-3fd8c2b92732","_cell_guid":"5a63c59f-3a8d-4153-b4f4-c2b5589a0f03","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:31:38.296371Z","iopub.execute_input":"2023-10-02T16:31:38.297237Z","iopub.status.idle":"2023-10-02T16:31:38.310499Z","shell.execute_reply.started":"2023-10-02T16:31:38.297202Z","shell.execute_reply":"2023-10-02T16:31:38.309220Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing outliers for the 'aortic_hu' column\nplt.figure(figsize=(12, 8))\nsns.boxplot(train_meta['aortic_hu'])\nplt.axhline(lower_bound, color='r', linestyle='--', label=f\"Lower Bound: {lower_bound}\")\nplt.axhline(upper_bound, color='g', linestyle='--', label=f\"Upper Bound: {upper_bound}\")\nplt.title('Boxplot of Aortic HU with Outliers Highlighted')\nplt.xlabel('Aortic HU')\nplt.legend()\nplt.show()","metadata":{"_uuid":"c2bf69d2-cf74-45cd-8b99-1e5732d26aa9","_cell_guid":"44798b9b-40be-4464-b16a-fc4cf01431b8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:31:38.311975Z","iopub.execute_input":"2023-10-02T16:31:38.312900Z","iopub.status.idle":"2023-10-02T16:31:38.569364Z","shell.execute_reply.started":"2023-10-02T16:31:38.312864Z","shell.execute_reply":"2023-10-02T16:31:38.568424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the plot, we can observe a cluster of data points above the upper bound, indicating potential outliers with higher aortic_hu values.","metadata":{"_uuid":"e7756723-955e-4ef9-bd7f-a2f89c4b580a","_cell_guid":"58fd8705-459b-4694-8108-29f724cce3eb","trusted":true}},{"cell_type":"markdown","source":"**Relationship Analysis:**","metadata":{"_uuid":"bba62ba8-ccd0-4201-a6ab-c2b2240a1d80","_cell_guid":"e3fd9fce-b6b4-4ba5-be8f-236bd559fe31","trusted":true}},{"cell_type":"markdown","source":"**1. Injury Type vs. Aortic HU:**","metadata":{"_uuid":"2ba638d4-7405-4935-b759-d2217c83266c","_cell_guid":"c64c83fe-9924-4284-9a5f-e20e6cd322d4","trusted":true}},{"cell_type":"code","source":"# Merging the 'label' and 'train_meta' datasets on 'patient_id' and 'series_id'\nmerged_data = pd.merge(labels, train_meta, on=['patient_id', 'series_id'])\n\n# Visualizing the distribution of 'aortic_hu' based on 'injury_name'\nplt.figure(figsize=(12, 8))\nsns.boxplot(data=merged_data, x='injury_name', y='aortic_hu')\nplt.title('Distribution of Aortic HU based on Injury Type')\nplt.xlabel('Injury Type')\nplt.ylabel('Aortic HU')\nplt.show()","metadata":{"_uuid":"ff007837-cb2f-4559-9ab6-edcd015bbf89","_cell_guid":"0f487548-721e-411e-9ea0-1deff37d1da8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:31:38.570577Z","iopub.execute_input":"2023-10-02T16:31:38.571467Z","iopub.status.idle":"2023-10-02T16:31:38.848305Z","shell.execute_reply.started":"2023-10-02T16:31:38.571433Z","shell.execute_reply":"2023-10-02T16:31:38.847222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For bowel_injury, the distribution appears to have a slightly higher median and is more compact in terms of the interquartile range (IQR) compared to extravasation.\nThe extravasation injury (which represents active bleeding) has a broader IQR, indicating more variability in the aortic_hu values for this injury type. There are also a few potential outliers present for this injury type.","metadata":{"_uuid":"cd10d8fc-1140-4133-96e5-8325050b46d2","_cell_guid":"df412913-8ece-4882-a583-58e72892a371","trusted":true}},{"cell_type":"markdown","source":"**2. Injury Type vs. Completeness of Organ:**","metadata":{"_uuid":"6f052404-b26b-447b-8650-e133a8bc08c2","_cell_guid":"a92498de-e06e-43c2-aa2b-8eeeefca0069","trusted":true}},{"cell_type":"code","source":"# Visualizing the relationship between 'injury_name' and 'incomplete_organ'\nplt.figure(figsize=(10, 6))\nsns.countplot(data=merged_data, x='injury_name', hue='incomplete_organ')\nplt.title('Injury Type vs. Completeness of Organ')\nplt.xlabel('Injury Type')\nplt.ylabel('Count')\nplt.legend(title='Incomplete Organ (0 = Complete, 1 = Incomplete)')\nplt.show()","metadata":{"_uuid":"f55b52e8-3696-4724-a398-ae4e9ff114d7","_cell_guid":"03448a94-7e64-446c-b5f1-1c81013892f0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:31:38.849718Z","iopub.execute_input":"2023-10-02T16:31:38.850649Z","iopub.status.idle":"2023-10-02T16:31:39.118551Z","shell.execute_reply.started":"2023-10-02T16:31:38.850612Z","shell.execute_reply":"2023-10-02T16:31:39.117606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For both bowel_injury and extravasation injury types, the majority of the organs in the images are complete (incomplete_organ = 0).\nThe number of images with incomplete organs (incomplete_organ = 1) is relatively lower for both injury types, with extravasation having a slightly higher count of incomplete organs compared to bowel_injury.","metadata":{"_uuid":"b747a93c-4f28-4eb9-bfc8-ba559491db41","_cell_guid":"7cf221dd-d265-4efe-baec-1e2b38b51580","trusted":true}},{"cell_type":"markdown","source":"## Importing Images","metadata":{"_uuid":"2b4dc373-6952-48b6-81dc-27c0f1d4c6b3","_cell_guid":"9da7eddd-a5f0-4023-a8f2-846208d7db5d","trusted":true}},{"cell_type":"markdown","source":"Start by creating image paths for test dataset","metadata":{"_uuid":"230fbbef-3bfa-4ce1-9d2d-044b18e2271c","_cell_guid":"1b2fed75-3918-4ff1-a486-a0e8e4f5d541","trusted":true}},{"cell_type":"code","source":"# Adjusting the path generation function to exclude instance_number\ndef test_img_path(row):\n    return f\"/kaggle/input/rsna-2023-abdominal-trauma-detection/test_images/{row['patient_id']}/{row['series_id']}/\"\n\ntest_meta['test_img_path'] = test_meta.apply(test_img_path, axis=1)\n\n# Display the first few rows of the test_meta dataframe with the new 'adjusted_img_path' column\ntest_meta.head()","metadata":{"_uuid":"adaff152-31d2-4061-a7a5-38f50b277a98","_cell_guid":"d0e0de97-c59b-43dd-9214-cba235f7de8c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:31:39.119779Z","iopub.execute_input":"2023-10-02T16:31:39.120685Z","iopub.status.idle":"2023-10-02T16:31:39.133329Z","shell.execute_reply.started":"2023-10-02T16:31:39.120650Z","shell.execute_reply":"2023-10-02T16:31:39.132195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Craeting image path for train dataset","metadata":{"_uuid":"5a195148-0e73-48fa-a42d-048b1bb24fb7","_cell_guid":"bdbc3cfb-e2dd-4352-b8b3-7715ff65d590","trusted":true}},{"cell_type":"code","source":"def img_path(row):\n    return f\"/kaggle/input/rsna-2023-abdominal-trauma-detection/train_images/{row['patient_id']}/{row['series_id']}/{row['instance_number']}.dcm\"\n\nlabels['img_path'] = labels.apply(img_path, axis=1)","metadata":{"_uuid":"c082d0f1-3fc0-4aa1-adc8-aa8f0c5cacf7","_cell_guid":"6dcb55d9-61ca-4440-b39e-8fe414a82913","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:31:39.138882Z","iopub.execute_input":"2023-10-02T16:31:39.139829Z","iopub.status.idle":"2023-10-02T16:31:39.254222Z","shell.execute_reply.started":"2023-10-02T16:31:39.139793Z","shell.execute_reply":"2023-10-02T16:31:39.253218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**DICOM Image Visualization:**","metadata":{"_uuid":"f82610f3-fcf5-4e10-87c7-18bffc190183","_cell_guid":"5fa8e64c-645a-4a3e-a73d-0979ae31611a","trusted":true}},{"cell_type":"code","source":"# Generating Kaggle reference paths for the 'train' dataset again\nlabels['img_path'] = labels.apply(img_path, axis=1)\n\n# Displaying the first few rows of the 'train' dataset with the updated 'img_path' column\nlabels.head()","metadata":{"_uuid":"27ae19ca-a981-40b1-baed-1951e9f47e58","_cell_guid":"0bb616b3-6023-4275-8723-33c9751b4753","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:31:39.255569Z","iopub.execute_input":"2023-10-02T16:31:39.256122Z","iopub.status.idle":"2023-10-02T16:31:39.377400Z","shell.execute_reply.started":"2023-10-02T16:31:39.256089Z","shell.execute_reply":"2023-10-02T16:31:39.376347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ramdomly display injury type and image","metadata":{"_uuid":"6cf87906-481a-4011-9d9e-dbe2c329709d","_cell_guid":"b01d722c-a20a-4ea9-b394-e0589a4d1103","trusted":true}},{"cell_type":"code","source":"import pydicom\nimport matplotlib.pyplot as plt\n\ndef read_dicom_image(path):\n    \"\"\"\n    Reads a DICOM image and returns its pixel array.\n    \"\"\"\n    dicom_img = pydicom.dcmread(path)\n    return dicom_img.pixel_array\n\n# Sample 20 rows from the train dataset\nsample_data = labels.sample(20)\n\n# Extract the img_paths and corresponding injury names for labeling\nsample_img_paths = sample_data['img_path'].tolist()\nsample_labels = sample_data['injury_name'].tolist()\n\n# Set up the figure for visualization\nplt.figure(figsize=(15, 30))\n\n# Loop through the sampled image paths and display them in rows of 3 with labels\nfor idx, (img_path, label) in enumerate(zip(sample_img_paths, sample_labels), start=1):\n    plt.subplot(7, 3, idx)  # 7 rows, 3 columns\n    plt.imshow(read_dicom_image(img_path), cmap='gray')\n    plt.title(label)\n    plt.axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"d8e640ad-436f-40b4-896b-ff535e15093b","_cell_guid":"dc8b8fc4-40b2-491a-b34e-df15c6185377","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:31:39.379137Z","iopub.execute_input":"2023-10-02T16:31:39.379554Z","iopub.status.idle":"2023-10-02T16:31:42.135435Z","shell.execute_reply.started":"2023-10-02T16:31:39.379514Z","shell.execute_reply":"2023-10-02T16:31:42.133098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**comparison of images for each injury type**","metadata":{"_uuid":"24223b9f-662e-4a08-b855-f693bbb0f7c4","_cell_guid":"ffe3f583-c283-4995-a115-3d1001aca7c0","trusted":true}},{"cell_type":"code","source":"import pydicom\nimport matplotlib.pyplot as plt\n\ndef read_dicom_image(path):\n    \"\"\"\n    Reads a DICOM image and returns its pixel array.\n    \"\"\"\n    dicom_img = pydicom.dcmread(path)\n    return dicom_img.pixel_array\n\n# Sample one image path for each injury type\nsample_img_paths = labels.groupby('injury_name').apply(lambda x: x.sample(1)['img_path'].values[0])\nsample_labels = sample_img_paths.index.tolist()\n\n# Set up the figure for visualization\nplt.figure(figsize=(15, 5))\n\n# Loop through the sampled image paths and display them side by side with labels\nfor idx, (img_path, label) in enumerate(zip(sample_img_paths, sample_labels), start=1):\n    plt.subplot(1, len(sample_img_paths), idx)\n    plt.imshow(read_dicom_image(img_path), cmap='gray')\n    plt.title(label)\n    plt.axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"d094eaeb-f5f7-4a4b-9190-2eb921ee59eb","_cell_guid":"84fde876-b399-4255-90b0-0c20074b7d91","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:31:42.136856Z","iopub.execute_input":"2023-10-02T16:31:42.137418Z","iopub.status.idle":"2023-10-02T16:31:42.570444Z","shell.execute_reply.started":"2023-10-02T16:31:42.137381Z","shell.execute_reply":"2023-10-02T16:31:42.569504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Randomly Display Images by Patient ID","metadata":{"_uuid":"7ca7d420-b75e-462e-8151-b5f4d409e132","_cell_guid":"bb4eda34-6b30-44ca-9099-649331a61672","trusted":true}},{"cell_type":"code","source":"import pydicom\nimport matplotlib.pyplot as plt\nimport random\n\ndef read_dicom_image(path):\n    \"\"\"\n    Reads a DICOM image and returns its pixel array.\n    \"\"\"\n    dicom_img = pydicom.dcmread(path)\n    return dicom_img.pixel_array\n\n# Get unique patient IDs from your DataFrame\nunique_patient_ids = labels['patient_id'].unique()\n\n# Randomly select 5 patient IDs (or you can select a fixed set)\nrandom_patient_ids = random.sample(list(unique_patient_ids), 5)\n\n# Set up a grid for displaying images\nnum_rows = 5  # Number of rows in the grid (one row per patient)\nnum_cols = 5  # Number of columns in the grid (up to 5 images per patient)\nplt.figure(figsize=(15, 10))\n\n# Iterate through randomly selected patient IDs\nfor row, random_patient_id in enumerate(random_patient_ids, start=1):\n    # Filter the DataFrame to get all images for the randomly selected patient\n    patient_images = labels[labels['patient_id'] == random_patient_id]\n    \n    # Get unique series IDs for the patient\n    unique_series_ids = patient_images['series_id'].unique()\n    \n    # Randomly select up to 5 unique series IDs (you can adjust the number)\n    random_series_ids = random.sample(list(unique_series_ids), min(5, len(unique_series_ids)))\n    \n    # Iterate through randomly selected series IDs for the patient\n    for col, random_series_id in enumerate(random_series_ids, start=1):\n        # Filter the DataFrame to get all images for the selected series\n        series_images = patient_images[patient_images['series_id'] == random_series_id]\n        \n        # Display each image in the series\n        for i, (_, image_row) in enumerate(series_images.iterrows(), start=1):\n            image_path = image_row['img_path']\n            plt.subplot(num_rows, num_cols, (row - 1) * num_cols + col)\n            plt.imshow(read_dicom_image(image_path), cmap='gray')\n            plt.title(f'Patient ID: {random_patient_id}\\nSeries ID: {random_series_id}\\nImage {i}')\n            plt.axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"7f7adf32-f988-4c60-8452-d7439371aef5","_cell_guid":"a0949bdb-78ac-4609-9f8f-756140a3092f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:31:42.571846Z","iopub.execute_input":"2023-10-02T16:31:42.572384Z","iopub.status.idle":"2023-10-02T16:31:50.664466Z","shell.execute_reply.started":"2023-10-02T16:31:42.572350Z","shell.execute_reply":"2023-10-02T16:31:50.663533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**preprocessing :**\n\n\n* Rescaling: Adjusting the intensity values to a standard scale, e.g., between 0 and 1.\n* Resizing: Making sure all images have the same size, especially if they are being fed into a neural network.\n* Histogram Equalization: Enhancing the contrast of images.\n* Normalization: Removing the mean and scaling to unit variance.\n* Data Augmentation: Techniques such as rotation, zooming, and flipping to artificially increase the size of the dataset (useful for training deep learning models).\n* Smoothing\n*Padding","metadata":{"_uuid":"8ad7f65b-5c97-4a1a-b06f-c5e99674bc84","_cell_guid":"7eec1450-2d51-435b-8af0-53b1b20da5e4","trusted":true}},{"cell_type":"code","source":"import pydicom\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load a sample DICOM image\nsample_path = labels['img_path'].iloc[0]\ndicom_img = pydicom.dcmread(sample_path).pixel_array\n\n# Rescale the image to the range [0, 1]\nrescaled_img = cv2.normalize(dicom_img, None, 0, 1, cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n\n# Apply histogram equalization\nequalized_img = cv2.equalizeHist((rescaled_img * 255).astype(np.uint8))\n\n# Plot original and preprocessed images side by side\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.imshow(dicom_img, cmap='gray')\nplt.title('Original Image')\nplt.subplot(1, 2, 2)\nplt.imshow(equalized_img, cmap='gray')\nplt.title('Preprocessed Image')\nplt.show()","metadata":{"_uuid":"1fc60d2c-5dca-45cc-9122-656b43f2e281","_cell_guid":"f4eac7eb-d9a9-4ac9-b3ab-9da382880a2b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:31:50.665929Z","iopub.execute_input":"2023-10-02T16:31:50.666523Z","iopub.status.idle":"2023-10-02T16:31:51.148358Z","shell.execute_reply.started":"2023-10-02T16:31:50.666488Z","shell.execute_reply":"2023-10-02T16:31:51.147455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pydicom\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Function to read a DICOM image and return its pixel array\ndef read_dicom_image(path):\n    dicom_img = pydicom.dcmread(path)\n    return dicom_img.pixel_array\n\n# Load a sample DICOM image\nsample_path = labels['img_path'].iloc[0]\ndicom_img = read_dicom_image(sample_path)\n\n# Rescale the image to the range [0, 1]\nrescaled_img = cv2.normalize(dicom_img, None, 0, 1, cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n\n# Apply histogram equalization\nequalized_img = cv2.equalizeHist((rescaled_img * 255).astype(np.uint8))\n\n# Apply Gaussian smoothing\nk_size = (5, 5)  # Kernel size for Gaussian filter\nsigma = 0.5      # Standard deviation for Gaussian filter\nsmoothed_img = cv2.GaussianBlur(equalized_img, k_size, sigma)\n\n# Define padding size (top, bottom, left, right)\npadding_size = (20, 20, 20, 20)\n\n# Apply zero-padding\npadded_img = np.pad(smoothed_img, ((padding_size[0], padding_size[1]), (padding_size[2], padding_size[3])), mode='constant', constant_values=0)\n\n# Plot original, rescaled, equalized, smoothed, and padded images\nplt.figure(figsize=(20, 8))\nplt.subplot(1, 5, 1)\nplt.imshow(dicom_img, cmap='gray')\nplt.title('Original Image')\n\nplt.subplot(1, 5, 2)\nplt.imshow(rescaled_img, cmap='gray')\nplt.title('Rescaled Image')\n\nplt.subplot(1, 5, 3)\nplt.imshow(equalized_img, cmap='gray')\nplt.title('Equalized Image')\n\nplt.subplot(1, 5, 4)\nplt.imshow(smoothed_img, cmap='gray')\nplt.title('Smoothed Image')\n\nplt.subplot(1, 5, 5)\nplt.imshow(padded_img, cmap='gray')\nplt.title('Padded Image')\n\nplt.show()","metadata":{"_uuid":"42e18ee9-b63f-4b60-9065-6ba2f9df7570","_cell_guid":"9540ffa5-1d94-4843-89c1-f48f39aeeee7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:31:51.149909Z","iopub.execute_input":"2023-10-02T16:31:51.150515Z","iopub.status.idle":"2023-10-02T16:31:52.071276Z","shell.execute_reply.started":"2023-10-02T16:31:51.150482Z","shell.execute_reply":"2023-10-02T16:31:52.070438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pydicom\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport gc\n\ndef read_dicom_image(path):\n    dicom_img = pydicom.dcmread(path)\n    return dicom_img.pixel_array\n\ndef process_image(img):\n    rescaled_img = cv2.normalize(img, None, 0, 1, cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n    equalized_img = cv2.equalizeHist((rescaled_img * 255).astype(np.uint8))\n    k_size = (5, 5)\n    sigma = 0.5\n    smoothed_img = cv2.GaussianBlur(equalized_img, k_size, sigma)\n    padding_size = (20, 20, 20, 20)\n    padded_img = np.pad(smoothed_img, ((padding_size[0], padding_size[1]), (padding_size[2], padding_size[3])), mode='constant', constant_values=0)\n    \n    # Resize to a fixed size\n    resized_img = cv2.resize(padded_img, (256, 256))\n    \n    return resized_img / 255.0  # normalize to [0,1]\n\ndef process_batch(batch):\n    batch_images = []\n    for index, row in batch.iterrows():\n        img = read_dicom_image(row['img_path'])\n        processed_img = process_image(img)\n        batch_images.append(processed_img)\n    return np.stack(batch_images)\n\ndef image_generator(labels_df, batch_size):\n    num_samples = len(labels_df)\n    \n    while True:\n        for start in range(0, num_samples, batch_size):\n            end = min(start + batch_size, num_samples)\n            batch = labels_df.iloc[start:end]\n            batch_images = process_batch(batch)\n            \n            yield batch_images\n            \n            # Free up memory\n            del batch_images\n            gc.collect()\n\n# Sample usage\nbatch_size = 100\ndata_gen = image_generator(labels, batch_size=batch_size)\n\n# To get the next batch of images:\n# next_batch = next(data_gen)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T16:31:52.072644Z","iopub.execute_input":"2023-10-02T16:31:52.073148Z","iopub.status.idle":"2023-10-02T16:31:52.086347Z","shell.execute_reply.started":"2023-10-02T16:31:52.073116Z","shell.execute_reply":"2023-10-02T16:31:52.085032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels.head()","metadata":{"_uuid":"16babf0e-3751-4f2f-b484-348b582e5b0d","_cell_guid":"bcb1c2b0-b990-476c-ae7a-3ae39d913bc6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:31:52.087933Z","iopub.execute_input":"2023-10-02T16:31:52.088999Z","iopub.status.idle":"2023-10-02T16:31:52.109142Z","shell.execute_reply.started":"2023-10-02T16:31:52.088966Z","shell.execute_reply":"2023-10-02T16:31:52.108070Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Modelling**","metadata":{"_uuid":"5f0be13b-c3e5-4279-803e-efdc417dd210","_cell_guid":"732a5d52-b083-4a89-b769-1378b48ad72a","trusted":true}},{"cell_type":"markdown","source":"**Steps for Model Building:**\n* Data Preparation: Split the data into training and validation sets.\n* Data Augmentation: Use data augmentation techniques to artificially increase the size of the training dataset.\n* Model Architecture: Define the CNN architecture.\n* Model Compilation: Specify the loss function, optimizer, and metrics.\n* Model Training: Train the model using the training data.\n* Model Evaluation: Evaluate the model's performance on the validation data.","metadata":{"_uuid":"7fe640db-f07f-4c9a-81a5-4ff2aeed87b6","_cell_guid":"f94bc5e9-50e9-43b8-8990-23b9fbdb6383","trusted":true}},{"cell_type":"markdown","source":"","metadata":{"_uuid":"23bfa93e-708c-456c-a65c-aa2223a0362c","_cell_guid":"632da0e9-309c-48a7-a983-bf57e95b533a","trusted":true}},{"cell_type":"markdown","source":"Merging Pre-Processed Labels and Train CSV","metadata":{"_uuid":"34dcf72e-0e8f-4441-a0b3-54c5b4ba954c","_cell_guid":"c657c516-98e9-4bc9-a1db-f538e29d4988","trusted":true}},{"cell_type":"code","source":"\nmodel_df = pd.merge(train, labels, on='patient_id', how='inner')\nmodel_df.shape","metadata":{"_uuid":"4599d4db-0b90-4f67-ad64-72c4c02c8084","_cell_guid":"cf1ece93-a0a0-4e46-acef-c70ac2790f6e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:31:52.110607Z","iopub.execute_input":"2023-10-02T16:31:52.111519Z","iopub.status.idle":"2023-10-02T16:31:52.128468Z","shell.execute_reply.started":"2023-10-02T16:31:52.111487Z","shell.execute_reply":"2023-10-02T16:31:52.127152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_df = model_df.drop_duplicates()\nmodel_df.shape","metadata":{"_uuid":"9e59f7fb-6148-4a06-a674-e57e9743c764","_cell_guid":"71480e9f-091d-484e-b52c-4bec33abf9c7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:31:52.129985Z","iopub.execute_input":"2023-10-02T16:31:52.131193Z","iopub.status.idle":"2023-10-02T16:31:52.150544Z","shell.execute_reply.started":"2023-10-02T16:31:52.131157Z","shell.execute_reply":"2023-10-02T16:31:52.149437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**1. Modelling**","metadata":{"_uuid":"bca257dc-e7c0-4981-af86-2657265d55f1","_cell_guid":"3891899f-3317-4501-beed-c85c6ba311db","trusted":true}},{"cell_type":"markdown","source":"**Data Preparation**: Split the data into training and validation sets.","metadata":{"_uuid":"18c9629c-9e4e-4128-917e-e92d1cc8ed45","_cell_guid":"481bd68d-35fd-4fa1-895b-91c2e346b85c","trusted":true}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndef batch_generator(data_df, batch_size):\n    while True:\n        for start in range(0, len(data_df), batch_size):\n            end = min(start + batch_size, len(data_df))\n            batch_df = data_df.iloc[start:end]\n            batch_images = process_batch(batch_df)\n            batch_labels = batch_df[y_train_columns].values\n            yield batch_images, batch_labels\n\n# Splitting the data into training (80%) and validation (20%) sets again\ntrain_patient_ids, val_patient_ids = train_test_split(model_df['patient_id'].unique(), test_size=0.2, random_state=42)\n\n# Use boolean indexing to filter rows in model_df based on patient IDs\ntrain_df = model_df[model_df['patient_id'].isin(train_patient_ids)]\nval_df = model_df[model_df['patient_id'].isin(val_patient_ids)]\n\n# Define column names for labels\ny_train_columns = [ 'any_injury']\n\n# Create generators for training and validation\nbatch_size = 32\ntrain_gen = batch_generator(train_df, batch_size=batch_size)\nval_gen = batch_generator(val_df, batch_size=batch_size)\n\n# Number of steps per epoch\ntrain_steps_per_epoch = len(train_df) // batch_size\nval_steps_per_epoch = len(val_df) // batch_size\n\n# Sample training loop\n# history = model.fit(train_gen, \n#                     steps_per_epoch=train_steps_per_epoch,\n#                     validation_data=val_gen, \n#                     validation_steps=val_steps_per_epoch, \n#                     epochs=10)\n","metadata":{"_uuid":"2a684805-4963-45e2-91d5-a925005f9d5e","_cell_guid":"6d0f178a-40ad-4ec1-961b-6e3bd1738f0f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:31:52.152050Z","iopub.execute_input":"2023-10-02T16:31:52.152831Z","iopub.status.idle":"2023-10-02T16:31:52.164736Z","shell.execute_reply.started":"2023-10-02T16:31:52.152778Z","shell.execute_reply":"2023-10-02T16:31:52.163652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"_uuid":"4fd45d1c-5dcd-4d29-9948-8e34e1817ba7","_cell_guid":"5a16d173-b64b-4337-8d59-ed4156cc0143","trusted":true}},{"cell_type":"code","source":"# Convert the train and val patient IDs to sets\ntrain_patient_ids_set = set(train_patient_ids)\nval_patient_ids_set = set(val_patient_ids)\n\n# Check for common patient IDs between train and val\ncommon_patient_ids = train_patient_ids_set.intersection(val_patient_ids_set)\n\n# Check if there are any common patient IDs\nif len(common_patient_ids) > 0:\n    print(\"Common Patient IDs between Train and Validation Sets:\")\n    print(common_patient_ids)\nelse:\n    print(\"No Common Patient IDs between Train and Validation Sets\")","metadata":{"_uuid":"a7ebf8bc-1c1d-4775-a2d7-c67eb64434e4","_cell_guid":"61ec9e30-a2ad-4115-ad16-8747953504df","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:31:52.166296Z","iopub.execute_input":"2023-10-02T16:31:52.166988Z","iopub.status.idle":"2023-10-02T16:31:52.178601Z","shell.execute_reply.started":"2023-10-02T16:31:52.166955Z","shell.execute_reply":"2023-10-02T16:31:52.177558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Image  Preprocessing**","metadata":{"_uuid":"6197055d-5ddc-41a5-bc9c-c4abbc0c0651","_cell_guid":"bf44c0bd-a515-44a8-aea5-04789beef1b5","trusted":true}},{"cell_type":"markdown","source":"**2. Model Architecture Definition**","metadata":{"_uuid":"e6771aab-bef8-4d8e-9cb1-c3fb955d03b9","_cell_guid":"0bce1d4e-b626-4bad-af1a-6dce3e2323af","trusted":true}},{"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\n\n# Create a model\nmodel = Sequential([\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 1)),  # Adjusted input shape\n    layers.MaxPooling2D(2, 2),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D(2, 2),\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.MaxPooling2D(2, 2),\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.MaxPooling2D(2, 2),\n    layers.Flatten(),\n    layers.Dropout(0.5),\n    layers.Dense(512, activation='relu'),\n    layers.Dense(len(y_train_columns), activation='sigmoid')  # Assuming y_train_columns is defined elsewhere in your code.\n])","metadata":{"_uuid":"2264f4ab-9668-459d-a8df-a0723fe8e49c","_cell_guid":"0fcee0b1-e972-4537-9448-cff479fd165b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:31:52.179821Z","iopub.execute_input":"2023-10-02T16:31:52.180704Z","iopub.status.idle":"2023-10-02T16:31:52.268150Z","shell.execute_reply.started":"2023-10-02T16:31:52.180671Z","shell.execute_reply":"2023-10-02T16:31:52.267183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3. Model Compilation**","metadata":{"_uuid":"78064819-0230-41f9-85b7-048943921e63","_cell_guid":"75c7a40c-7e74-4b51-8ed2-15647e837a4e","trusted":true}},{"cell_type":"code","source":"model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","metadata":{"_uuid":"fd7e5f46-c448-4357-949b-b2b85c562683","_cell_guid":"ed6cf54d-1674-4547-a946-8f071b768617","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:31:52.269407Z","iopub.execute_input":"2023-10-02T16:31:52.269960Z","iopub.status.idle":"2023-10-02T16:31:52.281549Z","shell.execute_reply.started":"2023-10-02T16:31:52.269927Z","shell.execute_reply":"2023-10-02T16:31:52.280609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**4. Model Training**","metadata":{"_uuid":"6c40bdac-9fa9-4c3d-b179-1af66b50b46d","_cell_guid":"4dcda938-8241-4cd6-aa32-df660b9d26b9","trusted":true}},{"cell_type":"code","source":"sample_batch_images, sample_batch_labels = next(train_gen)\nprint(sample_batch_images.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T16:31:52.282847Z","iopub.execute_input":"2023-10-02T16:31:52.283963Z","iopub.status.idle":"2023-10-02T16:31:52.651477Z","shell.execute_reply.started":"2023-10-02T16:31:52.283921Z","shell.execute_reply":"2023-10-02T16:31:52.650483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_gen, \n                    steps_per_epoch=train_steps_per_epoch,\n                    validation_data=val_gen, \n                    validation_steps=val_steps_per_epoch, \n                    epochs=10)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T16:31:52.652855Z","iopub.execute_input":"2023-10-02T16:31:52.653476Z","iopub.status.idle":"2023-10-02T16:56:06.816343Z","shell.execute_reply.started":"2023-10-02T16:31:52.653437Z","shell.execute_reply":"2023-10-02T16:56:06.815319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**5. Model Evaluation**","metadata":{"_uuid":"578a2a8d-8c1c-4e97-b39e-f7715f56b64d","_cell_guid":"79ae283d-e86d-40d0-9312-001aa81847d0","trusted":true}},{"cell_type":"code","source":"# Evaluate the model on the validation generator\nval_loss, val_accuracy = model.evaluate(val_gen, steps=val_steps_per_epoch)\n\nprint(f\"Validation Loss: {val_loss:.4f}\")\nprint(f\"Validation Accuracy: {val_accuracy:.4f}\")\n","metadata":{"_uuid":"bb8ef111-aaea-446c-aab9-e239f9387d6d","_cell_guid":"7195e602-506e-45dc-bb2f-73f2d354a99d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:56:52.537619Z","iopub.execute_input":"2023-10-02T16:56:52.537982Z","iopub.status.idle":"2023-10-02T16:57:21.009638Z","shell.execute_reply.started":"2023-10-02T16:56:52.537953Z","shell.execute_reply":"2023-10-02T16:57:21.008478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**6. Visualize Training and Validation Accuracy**","metadata":{"_uuid":"a81c6a04-9b0c-450b-a927-953dbed0c64c","_cell_guid":"6fefcdf4-37ef-41d4-8765-0c08fb987e3a","trusted":true}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Extract accuracy values from the history object\ntrain_acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\n# Extract loss values from the history object (optional if you want to plot loss as well)\ntrain_loss = history.history['loss']\nval_loss = history.history['val_loss']\n\n# Create a range of values for epochs (starting from 1)\nepochs = range(1, len(train_acc) + 1)\n\n# Plot training and validation accuracy\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(epochs, train_acc, 'bo', label='Training Accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\n# Plot training and validation loss (optional)\nplt.subplot(1, 2, 2)\nplt.plot(epochs, train_loss, 'bo', label='Training Loss')\nplt.plot(epochs, val_loss, 'b', label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n","metadata":{"_uuid":"671227ae-daf5-435e-9814-9b40f1527f76","_cell_guid":"9e8eaf19-cd18-4a70-9ea0-698dfc4feebf","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:57:49.458187Z","iopub.execute_input":"2023-10-02T16:57:49.459238Z","iopub.status.idle":"2023-10-02T16:57:50.011330Z","shell.execute_reply.started":"2023-10-02T16:57:49.459197Z","shell.execute_reply":"2023-10-02T16:57:50.010245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"_uuid":"4e3f5add-e401-40b1-aa02-63670b76f392","_cell_guid":"162495e9-5a7b-43ef-936e-f3c3fd8d6f36","trusted":true}},{"cell_type":"code","source":"# Extract a batch of images and labels from the validation generator\nval_images, val_labels = next(val_gen)\n\n# Predict using the model\npredictions = model.predict(val_images)\n\n# Convert predictions to binary labels\npredicted_labels = (predictions > 0.5).astype(int)\n\n# Visualize the first few images, true labels, and predicted labels\nplt.figure(figsize=(15, 5))\nfor i in range(10):  # Displaying the first 10 images\n    plt.subplot(2, 5, i+1)\n    plt.imshow(val_images[i].squeeze(), cmap='gray')  # Assuming images are grayscale\n    plt.title(f\"True: {val_labels[i]}, Predicted: {predicted_labels[i]}\")\n    plt.axis('off')\nplt.tight_layout()\nplt.show()\n","metadata":{"_uuid":"291bd744-3ee6-47bb-b08a-1f23b39dbfa4","_cell_guid":"ff654d18-0db9-40cb-8da3-1849a1f572d8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T17:00:08.279094Z","iopub.execute_input":"2023-10-02T17:00:08.279540Z","iopub.status.idle":"2023-10-02T17:00:09.801939Z","shell.execute_reply.started":"2023-10-02T17:00:08.279502Z","shell.execute_reply":"2023-10-02T17:00:09.801060Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# Get all predictions and true labels from the validation set\nall_predictions = []\nall_true_labels = []\n\nfor i in range(val_steps_per_epoch):\n    val_images, val_labels = next(val_gen)\n    predictions = model.predict(val_images)\n    predicted_labels = (predictions > 0.5).astype(int)\n    all_predictions.extend(predicted_labels)\n    all_true_labels.extend(val_labels)\n\n# Generate the confusion matrix\ncm = confusion_matrix(all_true_labels, all_predictions)\n\n# Visualize the confusion matrix\nplt.figure(figsize=(6, 6))\nsns.heatmap(cm, annot=True, fmt='g', cmap='Blues', cbar=False)\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T17:00:36.248774Z","iopub.execute_input":"2023-10-02T17:00:36.249192Z","iopub.status.idle":"2023-10-02T17:01:09.930781Z","shell.execute_reply.started":"2023-10-02T17:00:36.249161Z","shell.execute_reply":"2023-10-02T17:01:09.929722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}