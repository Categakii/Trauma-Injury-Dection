{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Background Information:**\n\nTraumatic injury is a significant global health concern, especially affecting individuals in the first four decades of life. It is responsible for millions of annual deaths worldwide and poses a substantial public health challenge. Prompt and accurate diagnosis of traumatic injuries is crucial for improving patient outcomes and increasing survival rates. Among various diagnostic tools, computed tomography (CT) has emerged as a vital technology for evaluating individuals suspected of having abdominal injuries. CT scans provide detailed cross-sectional images of the abdomen, aiding in the detection and assessment of traumatic injuries.\n\n\nInterpreting CT scans for abdominal trauma can be a complex and time-consuming task, particularly when dealing with multiple injuries or subtle areas of active bleeding. This complexity often requires the expertise of medical professionals, and even for them, it can be challenging to make rapid and precise diagnoses. The need for timely intervention and appropriate treatment underscores the importance of improving the diagnostic process.\n\nArtificial intelligence (AI) and machine learning have demonstrated significant potential in assisting medical professionals in diagnosing and grading the severity of traumatic injuries. Advanced AI algorithms can enhance the speed and accuracy of detecting injuries, leading to improved trauma care and better patient outcomes on a global scale.\n\n### **Problem Statement:**\n\nTraumatic injury is a leading cause of death globally, particularly affecting individuals in their prime years. Timely and accurate diagnosis of traumatic injuries is essential for improving patient outcomes and reducing mortality rates. Computed tomography (CT) scans have emerged as a critical tool for evaluating individuals with suspected abdominal injuries due to their ability to provide detailed cross-sectional images. However, interpreting these CT scans, especially in cases of multiple injuries or subtle internal bleeding, poses a significant challenge for healthcare professionals, often leading to delays in diagnosis and treatment.\n\nTo address this pressing healthcare issue, the RSNA Abdominal Trauma Detection AI Challenge aims to harness the potential of artificial intelligence and machine learning. The challenge calls upon researchers to develop advanced AI algorithms capable of rapidly and precisely detecting severe injuries to internal abdominal organs, including the liver, kidneys, spleen, and bowel, as well as identifying active internal bleeding.\n\n\n### **Objectives:**\n\n*  To develop advanced AI algorithms capable of accurately detecting traumatic injuries in CT scans of the abdomen.\n* Classify the severity of detected injuries, providing valuable information for medical professionals.\n* To expedite the diagnosis process, enabling rapid identification of injuries and active bleeding.\n*  To improve trauma care and patient outcomes by providing AI tools that can assist medical professionals in making more accurate and timely diagnoses.\n\n### **Research Questions:**\n\n* To effectively  detect traumatic injuries to internal abdominal organs in CT scans?\n*  To identify features and patterns in CT images  that are  indicative of different types and severities of abdominal injuries?\n* To determine  the speed and accuracy of trauma diagnosis impact patient outcomes and survival rates?\n\n\n","metadata":{"_kg_hide-input":false,"_kg_hide-output":false}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport pydicom\nimport matplotlib.pyplot as plt\nimport cv2\nimport seaborn as sns\nimport tensorflow as tf\nimport os","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:31:20.983900Z","iopub.execute_input":"2023-09-21T15:31:20.984301Z","iopub.status.idle":"2023-09-21T15:31:20.990690Z","shell.execute_reply.started":"2023-09-21T15:31:20.984271Z","shell.execute_reply":"2023-09-21T15:31:20.989482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/rsna-2023-abdominal-trauma-detection/image_level_labels.csv')\nlabels = pd.read_csv('/kaggle/input/rsna-2023-abdominal-trauma-detection/train.csv')\ntrain_meta = pd.read_csv('/kaggle/input/rsna-2023-abdominal-trauma-detection/train_series_meta.csv')\ntest_meta = pd.read_csv('/kaggle/input/rsna-2023-abdominal-trauma-detection/test_series_meta.csv')\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:31:20.993141Z","iopub.execute_input":"2023-09-21T15:31:20.994046Z","iopub.status.idle":"2023-09-21T15:31:21.029219Z","shell.execute_reply.started":"2023-09-21T15:31:20.994007Z","shell.execute_reply":"2023-09-21T15:31:21.028375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Displaying the first few rows of each dataset\ntrain.head(), labels.head(), train_meta.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:31:21.030296Z","iopub.execute_input":"2023-09-21T15:31:21.031035Z","iopub.status.idle":"2023-09-21T15:31:21.046071Z","shell.execute_reply.started":"2023-09-21T15:31:21.031001Z","shell.execute_reply":"2023-09-21T15:31:21.044933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**train (image_level_labels.csv):**\n\n* patient_id: The unique identifier for each patient.\n* series_id: Identifier for the series of images for the patient.\n* instance_number: The specific image instance number within the series.\n* injury_name: The name or type of injury detected in the image.\n\n**labels (train.csv):**\n\n* This dataset provides the labels for different types of injuries for each patient.\n* Columns like bowel_healthy, bowel_injury, extravasation_healthy, etc., indicate the health status or injury severity of various organs for each patient.\n\n**train_meta (train_series_meta.csv):**\n\n* patient_id: The unique identifier for each patient.\n* series_id: Identifier for the series of images for the patient.\n* aortic_hu: A quantitative measure related to the images.\n* incomplete_organ: A binary indicator specifying whether the organ is incomplete in the images.\n\n\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"Basic Understanding","metadata":{}},{"cell_type":"code","source":"# Basic information for the 'train' dataset\ntrain_info = {\n    \"Number of Rows\": train.shape[0],\n    \"Number of Columns\": train.shape[1],\n    \"Columns\": train.columns.tolist(),\n    \"Data Types\": train.dtypes.tolist(),\n    \"Unique Values per Column\": train.nunique().tolist()\n}\n\n# Basic information for the 'labels' dataset\nlabels_info = {\n    \"Number of Rows\": labels.shape[0],\n    \"Number of Columns\": labels.shape[1],\n    \"Columns\": labels.columns.tolist(),\n    \"Data Types\": labels.dtypes.tolist(),\n    \"Unique Values per Column\": labels.nunique().tolist()\n}\n\n# Basic information for the 'train_meta' dataset\ntrain_meta_info = {\n    \"Number of Rows\": train_meta.shape[0],\n    \"Number of Columns\": train_meta.shape[1],\n    \"Columns\": train_meta.columns.tolist(),\n    \"Data Types\": train_meta.dtypes.tolist(),\n    \"Unique Values per Column\": train_meta.nunique().tolist()\n}\n\ntrain_info, labels_info, train_meta_info\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:31:21.047429Z","iopub.execute_input":"2023-09-21T15:31:21.047852Z","iopub.status.idle":"2023-09-21T15:31:21.070831Z","shell.execute_reply.started":"2023-09-21T15:31:21.047814Z","shell.execute_reply":"2023-09-21T15:31:21.069723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**1. train (image_level_labels.csv) Dataset:**\n* Number of Rows: 12,029\n* Number of Columns: 4\n* Columns:\n* patient_id: Identifier for the patient.\n* series_id: Identifier for the series of images for the patient.\n* instance_number: Specific image instance number within the series.\n* injury_name: Type of injury detected in the image.\n* Data Types: The data types are appropriate with integer types for identifiers and object (string) type for the injury name.\n* Unique Values: There are 246 unique patients, 330 unique series, and 925 unique instance numbers. The injury_name column has 2 unique values, indicating two types of injuries.\n\n**2. labels (train.csv) Dataset:**\n\n* Number of Rows: 3,147\n* Number of Columns: 15\n* Columns:\n* Various columns representing the health status or injury severity of various organs for each patient.\n* Data Types: All columns are of integer type.\n* Unique Values: There are 3,147 unique patients. The injury-related columns have binary values (0 or 1), indicating the absence or presence of a specific injury type.\n\n**3. train_meta (train_series_meta.csv) Dataset:**\n\n* Number of Rows: 4,711\n* Number of Columns: 4\n* Columns:\n* patient_id: Identifier for the patient.\n* series_id: Identifier for the series of images for the patient.\n* aortic_hu: A quantitative measure related to the images.\n* incomplete_organ: Binary indicator specifying whether the organ is incomplete in the images.\n* Data Types: The data types are appropriate with integer and float types.\n* Unique Values: There are 3,147 unique patients and 4,711 unique series. The incomplete_organ column has 2 unique values.\n\n\n\n\n","metadata":{}},{"cell_type":"code","source":"# Displaying missing values for each dataset separately\n\nmissing_values_train = train.isnull().sum().to_frame(name='Missing Values (train)')\nmissing_values_labels = labels.isnull().sum().to_frame(name='Missing Values (labels)')\nmissing_values_train_meta = train_meta.isnull().sum().to_frame(name='Missing Values (train_meta)')\n\nmissing_values_train, missing_values_labels, missing_values_train_meta\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:31:21.073508Z","iopub.execute_input":"2023-09-21T15:31:21.074276Z","iopub.status.idle":"2023-09-21T15:31:21.091346Z","shell.execute_reply.started":"2023-09-21T15:31:21.074226Z","shell.execute_reply":"2023-09-21T15:31:21.090250Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**1. train (image_level_labels.csv) Dataset:**\n\nNo missing values are present in any columns.\n\n**2. labels (train.csv) Dataset:**\n\nNo missing values are present in any columns.\n\n**3. train_meta (train_series_meta.csv) Dataset:**\n\nNo missing values are present in any columns.","metadata":{}},{"cell_type":"code","source":"# Checking for duplicates in each dataset\nduplicates_train = train.duplicated().sum()\nduplicates_labels = labels.duplicated().sum()\nduplicates_train_meta = train_meta.duplicated().sum()\n\nduplicates_summary = {\n    \"Dataset\": [\"train\", \"labels\", \"train_meta\"],\n    \"Number of Duplicates\": [duplicates_train, duplicates_labels, duplicates_train_meta]\n}\n\npd.DataFrame(duplicates_summary)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:31:21.092856Z","iopub.execute_input":"2023-09-21T15:31:21.093213Z","iopub.status.idle":"2023-09-21T15:31:21.113874Z","shell.execute_reply.started":"2023-09-21T15:31:21.093158Z","shell.execute_reply":"2023-09-21T15:31:21.112481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no duplicate rows in any of the datasets:\n\n* train (image_level_labels.csv) Dataset: 0 duplicates.\n* labels (train.csv) Dataset: 0 duplicates.\n* train_meta (train_series_meta.csv) Dataset: 0 duplicates.","metadata":{}},{"cell_type":"markdown","source":"## **Exploratory data Analysis**","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:20:23.208756Z","iopub.execute_input":"2023-09-21T11:20:23.209230Z","iopub.status.idle":"2023-09-21T11:20:23.217595Z","shell.execute_reply.started":"2023-09-21T11:20:23.209194Z","shell.execute_reply":"2023-09-21T11:20:23.215839Z"}}},{"cell_type":"code","source":"# Visualizing the distribution of injury types in the 'train' dataset\nplt.figure(figsize=(10, 6))\nsns.countplot(data=train, x='injury_name')\nplt.title('Distribution of Injury Types in train Dataset')\nplt.ylabel('Count')\nplt.xlabel('Injury Type')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:31:21.115344Z","iopub.execute_input":"2023-09-21T15:31:21.115699Z","iopub.status.idle":"2023-09-21T15:31:21.397762Z","shell.execute_reply.started":"2023-09-21T15:31:21.115663Z","shell.execute_reply":"2023-09-21T15:31:21.396528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data suggests that extravasation (active bleeding) is more frequently identified in the provided images than bowel injuries.","metadata":{}},{"cell_type":"code","source":"# Visualizing the distribution of injury-related columns in the 'labels' dataset\ninjury_columns = [col for col in labels.columns if col != \"patient_id\"]\ninjury_counts = labels[injury_columns].sum()\n\nplt.figure(figsize=(14, 8))\ninjury_counts.sort_values().plot(kind='barh')\nplt.title('Distribution of Injury-Related Columns in labels Dataset')\nplt.xlabel('Count')\nplt.ylabel('Injury Type / Health Status')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:31:21.399000Z","iopub.execute_input":"2023-09-21T15:31:21.399356Z","iopub.status.idle":"2023-09-21T15:31:21.807326Z","shell.execute_reply.started":"2023-09-21T15:31:21.399320Z","shell.execute_reply":"2023-09-21T15:31:21.806418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing the distribution of the 'aortic_hu' column in the 'train_meta' dataset\nplt.figure(figsize=(10, 6))\nsns.histplot(train_meta['aortic_hu'], bins=50, kde=True)\nplt.title('Distribution of Aortic HU in train_meta Dataset')\nplt.xlabel('Aortic HU')\nplt.ylabel('Count')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:31:21.809047Z","iopub.execute_input":"2023-09-21T15:31:21.809466Z","iopub.status.idle":"2023-09-21T15:31:22.314143Z","shell.execute_reply.started":"2023-09-21T15:31:21.809434Z","shell.execute_reply":"2023-09-21T15:31:22.313050Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"Hounsfield Units (HU) are a measure used in CT scans to describe radiodensity, and the distribution gives us an idea of the variation in these values across different images.","metadata":{}},{"cell_type":"markdown","source":"**Relationship Analysis:**","metadata":{}},{"cell_type":"code","source":"# Visualizing the relationship between 'aortic_hu' and 'incomplete_organ' in the 'train_meta' dataset\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=train_meta, x='incomplete_organ', y='aortic_hu')\nplt.title('Relationship between Aortic HU and Incomplete Organ in train_meta Dataset')\nplt.xlabel('Incomplete Organ (0 = Complete, 1 = Incomplete)')\nplt.ylabel('Aortic HU')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:31:22.318654Z","iopub.execute_input":"2023-09-21T15:31:22.319142Z","iopub.status.idle":"2023-09-21T15:31:22.596095Z","shell.execute_reply.started":"2023-09-21T15:31:22.319109Z","shell.execute_reply":"2023-09-21T15:31:22.595012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This suggests that there might be some relationship between the completeness of the organ in the image and the aortic_hu values.","metadata":{}},{"cell_type":"markdown","source":"**Outliers Analysis:**","metadata":{}},{"cell_type":"code","source":"# Outlier analysis for the 'aortic_hu' column using the IQR method\n\n# Calculate Q1, Q3, and IQR\nQ1 = train_meta['aortic_hu'].quantile(0.25)\nQ3 = train_meta['aortic_hu'].quantile(0.75)\nIQR = Q3 - Q1\n\n# Define bounds for outliers\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\n# Identify outliers\noutliers = train_meta[(train_meta['aortic_hu'] < lower_bound) | (train_meta['aortic_hu'] > upper_bound)]\n\n# Percentage of data points that are outliers\noutlier_percentage = (len(outliers) / len(train_meta)) * 100\n\noutlier_summary = {\n    \"Lower Bound\": lower_bound,\n    \"Upper Bound\": upper_bound,\n    \"Number of Outliers\": len(outliers),\n    \"Percentage of Outliers\": outlier_percentage\n}\n\noutlier_summary\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:31:22.597298Z","iopub.execute_input":"2023-09-21T15:31:22.597603Z","iopub.status.idle":"2023-09-21T15:31:22.611364Z","shell.execute_reply.started":"2023-09-21T15:31:22.597576Z","shell.execute_reply":"2023-09-21T15:31:22.610198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing outliers for the 'aortic_hu' column\nplt.figure(figsize=(12, 8))\nsns.boxplot(train_meta['aortic_hu'])\nplt.axhline(lower_bound, color='r', linestyle='--', label=f\"Lower Bound: {lower_bound}\")\nplt.axhline(upper_bound, color='g', linestyle='--', label=f\"Upper Bound: {upper_bound}\")\nplt.title('Boxplot of Aortic HU with Outliers Highlighted')\nplt.xlabel('Aortic HU')\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:31:22.612491Z","iopub.execute_input":"2023-09-21T15:31:22.613137Z","iopub.status.idle":"2023-09-21T15:31:22.926110Z","shell.execute_reply.started":"2023-09-21T15:31:22.613109Z","shell.execute_reply":"2023-09-21T15:31:22.924940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the plot, we can observe a cluster of data points above the upper bound, indicating potential outliers with higher aortic_hu values.","metadata":{}},{"cell_type":"markdown","source":"**Relationship Analysis:**","metadata":{}},{"cell_type":"markdown","source":"**1. Injury Type vs. Aortic HU:**","metadata":{}},{"cell_type":"code","source":"# Merging the 'train' and 'train_meta' datasets on 'patient_id' and 'series_id'\nmerged_data = pd.merge(train, train_meta, on=['patient_id', 'series_id'])\n\n# Visualizing the distribution of 'aortic_hu' based on 'injury_name'\nplt.figure(figsize=(12, 8))\nsns.boxplot(data=merged_data, x='injury_name', y='aortic_hu')\nplt.title('Distribution of Aortic HU based on Injury Type')\nplt.xlabel('Injury Type')\nplt.ylabel('Aortic HU')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:31:22.927868Z","iopub.execute_input":"2023-09-21T15:31:22.928637Z","iopub.status.idle":"2023-09-21T15:31:23.252872Z","shell.execute_reply.started":"2023-09-21T15:31:22.928595Z","shell.execute_reply":"2023-09-21T15:31:23.251774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For bowel_injury, the distribution appears to have a slightly higher median and is more compact in terms of the interquartile range (IQR) compared to extravasation.\nThe extravasation injury (which represents active bleeding) has a broader IQR, indicating more variability in the aortic_hu values for this injury type. There are also a few potential outliers present for this injury type.","metadata":{}},{"cell_type":"markdown","source":"**2. Injury Type vs. Completeness of Organ:**","metadata":{}},{"cell_type":"code","source":"# Visualizing the relationship between 'injury_name' and 'incomplete_organ'\nplt.figure(figsize=(10, 6))\nsns.countplot(data=merged_data, x='injury_name', hue='incomplete_organ')\nplt.title('Injury Type vs. Completeness of Organ')\nplt.xlabel('Injury Type')\nplt.ylabel('Count')\nplt.legend(title='Incomplete Organ (0 = Complete, 1 = Incomplete)')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:31:23.254140Z","iopub.execute_input":"2023-09-21T15:31:23.254961Z","iopub.status.idle":"2023-09-21T15:31:23.575160Z","shell.execute_reply.started":"2023-09-21T15:31:23.254931Z","shell.execute_reply":"2023-09-21T15:31:23.574094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For both bowel_injury and extravasation injury types, the majority of the organs in the images are complete (incomplete_organ = 0).\nThe number of images with incomplete organs (incomplete_organ = 1) is relatively lower for both injury types, with extravasation having a slightly higher count of incomplete organs compared to bowel_injury.","metadata":{}},{"cell_type":"markdown","source":"## Importing Images ","metadata":{}},{"cell_type":"code","source":"# Adjusting the path generation function to exclude instance_number\ndef test_img_path(row):\n    return f\"/kaggle/input/rsna-2023-abdominal-trauma-detection/test_images/{row['patient_id']}/{row['series_id']}/\"\n\ntest_meta['test_img_path'] = test_meta.apply(test_img_path, axis=1)\n\n# Display the first few rows of the test_meta dataframe with the new 'adjusted_img_path' column\ntest_meta.head()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:31:23.576735Z","iopub.execute_input":"2023-09-21T15:31:23.577396Z","iopub.status.idle":"2023-09-21T15:31:23.591459Z","shell.execute_reply.started":"2023-09-21T15:31:23.577362Z","shell.execute_reply":"2023-09-21T15:31:23.590501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def img_path(row):\n    return f\"/kaggle/input/rsna-2023-abdominal-trauma-detection/train_images/{row['patient_id']}/{row['series_id']}/{row['instance_number']}.dcm\"\n\ntrain['img_path'] = train.apply(img_path, axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:31:23.593151Z","iopub.execute_input":"2023-09-21T15:31:23.593493Z","iopub.status.idle":"2023-09-21T15:31:23.869567Z","shell.execute_reply.started":"2023-09-21T15:31:23.593467Z","shell.execute_reply":"2023-09-21T15:31:23.868650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**DICOM Image Visualization:**","metadata":{}},{"cell_type":"code","source":"# Generating Kaggle reference paths for the 'train' dataset again\ntrain['img_path'] = train.apply(img_path, axis=1)\n\n# Displaying the first few rows of the 'train' dataset with the updated 'img_path' column\ntrain.head()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:31:23.871549Z","iopub.execute_input":"2023-09-21T15:31:23.871890Z","iopub.status.idle":"2023-09-21T15:31:24.147428Z","shell.execute_reply.started":"2023-09-21T15:31:23.871859Z","shell.execute_reply":"2023-09-21T15:31:24.146337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pydicom\nimport matplotlib.pyplot as plt\n\ndef read_dicom_image(path):\n    \"\"\"\n    Reads a DICOM image and returns its pixel array.\n    \"\"\"\n    dicom_img = pydicom.dcmread(path)\n    return dicom_img.pixel_array\n\n# Sample 20 rows from the train dataset\nsample_data = train.sample(20)\n\n# Extract the img_paths and corresponding injury names for labeling\nsample_img_paths = sample_data['img_path'].tolist()\nsample_labels = sample_data['injury_name'].tolist()\n\n# Set up the figure for visualization\nplt.figure(figsize=(15, 30))\n\n# Loop through the sampled image paths and display them in rows of 3 with labels\nfor idx, (img_path, label) in enumerate(zip(sample_img_paths, sample_labels), start=1):\n    plt.subplot(7, 3, idx)  # 7 rows, 3 columns\n    plt.imshow(read_dicom_image(img_path), cmap='gray')\n    plt.title(label)\n    plt.axis('off')\n\nplt.tight_layout()\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:31:24.149298Z","iopub.execute_input":"2023-09-21T15:31:24.149628Z","iopub.status.idle":"2023-09-21T15:31:28.440739Z","shell.execute_reply.started":"2023-09-21T15:31:24.149599Z","shell.execute_reply":"2023-09-21T15:31:28.438064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**comparison of images for each injury type**","metadata":{}},{"cell_type":"code","source":"import pydicom\nimport matplotlib.pyplot as plt\n\ndef read_dicom_image(path):\n    \"\"\"\n    Reads a DICOM image and returns its pixel array.\n    \"\"\"\n    dicom_img = pydicom.dcmread(path)\n    return dicom_img.pixel_array\n\n# Sample one image path for each injury type\nsample_img_paths = train.groupby('injury_name').apply(lambda x: x.sample(1)['img_path'].values[0])\nsample_labels = sample_img_paths.index.tolist()\n\n# Set up the figure for visualization\nplt.figure(figsize=(15, 5))\n\n# Loop through the sampled image paths and display them side by side with labels\nfor idx, (img_path, label) in enumerate(zip(sample_img_paths, sample_labels), start=1):\n    plt.subplot(1, len(sample_img_paths), idx)\n    plt.imshow(read_dicom_image(img_path), cmap='gray')\n    plt.title(label)\n    plt.axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:31:28.441942Z","iopub.execute_input":"2023-09-21T15:31:28.442794Z","iopub.status.idle":"2023-09-21T15:31:28.991910Z","shell.execute_reply.started":"2023-09-21T15:31:28.442762Z","shell.execute_reply":"2023-09-21T15:31:28.990772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**preprocessing :**\n\n\n* Rescaling: Adjusting the intensity values to a standard scale, e.g., between 0 and 1.\n* Resizing: Making sure all images have the same size, especially if they are being fed into a neural network.\n* Histogram Equalization: Enhancing the contrast of images.\n* Normalization: Removing the mean and scaling to unit variance.\n* Data Augmentation: Techniques such as rotation, zooming, and flipping to artificially increase the size of the dataset (useful for training deep learning models).","metadata":{}},{"cell_type":"code","source":"import pydicom\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load a sample DICOM image\nsample_path = train['img_path'].iloc[0]\ndicom_img = pydicom.dcmread(sample_path).pixel_array\n\n# Rescale the image to the range [0, 1]\nrescaled_img = cv2.normalize(dicom_img, None, 0, 1, cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n\n# Apply histogram equalization\nequalized_img = cv2.equalizeHist((rescaled_img * 255).astype(np.uint8))\n\n# Plot original and preprocessed images side by side\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.imshow(dicom_img, cmap='gray')\nplt.title('Original Image')\nplt.subplot(1, 2, 2)\nplt.imshow(equalized_img, cmap='gray')\nplt.title('Preprocessed Image')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:31:28.993449Z","iopub.execute_input":"2023-09-21T15:31:28.993790Z","iopub.status.idle":"2023-09-21T15:31:29.608005Z","shell.execute_reply.started":"2023-09-21T15:31:28.993762Z","shell.execute_reply":"2023-09-21T15:31:29.606890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Modelling**","metadata":{}},{"cell_type":"markdown","source":"**Steps for Model Building:**\n* Data Preparation: Split the data into training and validation sets.\n* Data Augmentation: Use data augmentation techniques to artificially increase the size of the training dataset.\n* Model Architecture: Define the CNN architecture.\n* Model Compilation: Specify the loss function, optimizer, and metrics.\n* Model Training: Train the model using the training data.\n* Model Evaluation: Evaluate the model's performance on the validation data.","metadata":{}},{"cell_type":"markdown","source":"**1. Data Preparation**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Splitting the data into training (80%) and validation (20%) sets again\ntrain_data, val_data = train_test_split(train, test_size=0.2, stratify=train['injury_name'], random_state=42)\n\n# Extracting image paths and labels for training and validation sets\nX_train = train_data['img_path'].tolist()\ny_train = train_data['injury_name'].tolist()\n\nX_val = val_data['img_path'].tolist()\ny_val = val_data['injury_name'].tolist()\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:31:29.609674Z","iopub.execute_input":"2023-09-21T15:31:29.610360Z","iopub.status.idle":"2023-09-21T15:31:29.640753Z","shell.execute_reply.started":"2023-09-21T15:31:29.610319Z","shell.execute_reply":"2023-09-21T15:31:29.639634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. Data Augmentation","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Data Augmentation for the training set\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,           # Rescale pixel values to [0,1]\n    rotation_range=20,        # Randomly rotate the image (degrees, 0 to 180)\n    width_shift_range=0.2,    # Randomly shift images horizontally (fraction of total width)\n    height_shift_range=0.2,   # Randomly shift images vertically (fraction of total height)\n    horizontal_flip=True      # Randomly flip images horizontally\n)\n\n# Only rescaling for the validation set\nval_datagen = ImageDataGenerator(rescale=1./255)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:31:29.645098Z","iopub.execute_input":"2023-09-21T15:31:29.645569Z","iopub.status.idle":"2023-09-21T15:31:29.651513Z","shell.execute_reply.started":"2023-09-21T15:31:29.645537Z","shell.execute_reply":"2023-09-21T15:31:29.650512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting up generators to read images from the dataframe\ntrain_generator = train_datagen.flow_from_dataframe(\n    dataframe=train_data,\n    x_col=\"img_path\",\n    y_col=\"injury_name\",\n    target_size=(150, 150),\n    batch_size=32,\n    class_mode='categorical'\n)\n\nval_generator = val_datagen.flow_from_dataframe(\n    dataframe=val_data,\n    x_col=\"img_path\",\n    y_col=\"injury_name\",\n    target_size=(150, 150),\n    batch_size=32,\n    class_mode='categorical'\n)\n\n# To check class indices\nprint(train_generator.class_indices)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:31:29.652889Z","iopub.execute_input":"2023-09-21T15:31:29.653333Z","iopub.status.idle":"2023-09-21T15:31:29.732939Z","shell.execute_reply.started":"2023-09-21T15:31:29.653304Z","shell.execute_reply":"2023-09-21T15:31:29.732215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3. Model Architecture**","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n\nmodel = Sequential()\nmodel.add(Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))\nmodel.add(MaxPooling2D(2, 2))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(2, 2))\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(2, 2))\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dense(len(train_generator.class_indices), activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:31:29.734059Z","iopub.execute_input":"2023-09-21T15:31:29.734557Z","iopub.status.idle":"2023-09-21T15:31:29.966482Z","shell.execute_reply.started":"2023-09-21T15:31:29.734519Z","shell.execute_reply":"2023-09-21T15:31:29.965571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**4. Model Compilation**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}