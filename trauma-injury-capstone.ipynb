{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.17","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"The project aims to address the critical issue of prompt and accurate diagnosis of abdominal trauma, which is a common cause of death and a major public health concern globally. Abdominal trauma, often resulting from motor vehicle accidents, can lead to severe injuries to internal organs and internal bleeding\n### **Background Information:**\n\nTraumatic injury is a significant global health concern, especially affecting individuals in the first four decades of life. It is responsible for millions of annual deaths worldwide and poses a substantial public health challenge. Prompt and accurate diagnosis of traumatic injuries is crucial for improving patient outcomes and increasing survival rates. Among various diagnostic tools, computed tomography (CT) has emerged as a vital technology for evaluating individuals suspected of having abdominal injuries. CT scans provide detailed cross-sectional images of the abdomen, aiding in the detection and assessment of traumatic injuries.\n\n\nInterpreting CT scans for abdominal trauma can be a complex and time-consuming task, particularly when dealing with multiple injuries or subtle areas of active bleeding. This complexity often requires the expertise of medical professionals, and even for them, it can be challenging to make rapid and precise diagnoses. The need for timely intervention and appropriate treatment underscores the importance of improving the diagnostic process.\n\n\n\n### **Problem Statement:**\n\nWith more than 5 million deaths caused by traumatic injury each year, it is the largest cause of early-life mortality and a major public health concern worldwide. Among these, blunt abdominal trauma is frequently sustained in car accidents and can cause serious internal bleeding and damage. In Kenya, a country of over 50 million people, this challenge is magnified by the severe shortage of healthcare infrastructureâ€”only about 50 CT scanners and 200 trained radiologists are available nationwide. This shortage leads to misdiagnoses, delayed treatments due to average waiting times of several weeks, and a lack of access to vital healthcare services for many Kenyans. Despite government initiatives to invest in new CT scanners and train more radiologists, the need for rapid and accurate diagnosis remains critical. However, it is sometimes difficult and time-consuming for medical personnel to interpret CT scans for abdominal injuries. Therefore, there is an urgent need for automated, accurate, and rapid diagnostic solutions as any delay can be fatal.\n\n\n\n### **Objectives:**\n\n* To develop AI algorithms that can automatically and accurately detect traumatic injuries to internal abdominal organs using CT scans.\n\n* To classify the discovered injuries according to their severity, thereby providing medical experts a vital tool to start proper treatment.\n\n* To rigorously evaluate the developed algorithms using performance metrics that are relevant for both machine learning models and clinical applicability.\n\n\n\n\n\n### **Research Questions:**\n\n* How effective are AI algorithms in automatically detecting traumatic injuries to internal abdominal organs like the liver, kidneys, spleen, and bowel using CT scans?\n\n* What features and patterns in CT scans are most indicative of different severities of abdominal injuries, and how can they be utilized for automated injury grading?\n\n* What are the appropriate metrics for evaluating the performance of the developed AI algorithms in terms of both machine learning benchmarks and clinical utility?","metadata":{"_uuid":"f3a5fbbc-7fc5-439e-89ae-75a0ec8d9461","_cell_guid":"67f48caa-be20-4466-88d0-b66425dca0d1","_kg_hide-input":false,"_kg_hide-output":false,"trusted":true}},{"cell_type":"markdown","source":"### **Importing Libraries**","metadata":{"_uuid":"306118fe-088e-454f-943b-e0ebccc06e38","_cell_guid":"298ae7c8-47e7-4a0f-90e6-48c3bf245757","trusted":true}},{"cell_type":"code","source":"\nimport numpy as np \nimport pandas as pd\n#import pydicom\nimport matplotlib.pyplot as plt\nimport cv2\n#import seaborn as sns\nimport tensorflow as tf\nimport os","metadata":{"_uuid":"0fd7ed6c-6564-4023-84bd-bbcc483783bb","_cell_guid":"cf7ad05f-7572-4f5b-9d2a-5104b8efaeab","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Loading the datasets**","metadata":{"_uuid":"e7bfb316-0aa7-419f-97dd-5802f7094bbb","_cell_guid":"bbde70d8-ef5b-43a2-a3cb-209479b7ef94","trusted":true}},{"cell_type":"code","source":"labels = pd.read_csv('/kaggle/input/rsna-2023-abdominal-trauma-detection/image_level_labels.csv')\ntrain=pd.read_csv('/kaggle/input/rsna-2023-abdominal-trauma-detection/train.csv')\ntrain_meta = pd.read_csv('/kaggle/input/rsna-2023-abdominal-trauma-detection/train_series_meta.csv')\ntest_meta = pd.read_csv('/kaggle/input/rsna-2023-abdominal-trauma-detection/test_series_meta.csv')","metadata":{"_uuid":"5cdbdc25-b924-42bb-8118-e95cef92c03c","_cell_guid":"c1595fa7-c344-4cfd-b8bb-85ce8d2b8d9a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Displaying the first few rows of each dataset\ntrain.head(), labels.head(), train_meta.head()","metadata":{"_uuid":"13ada4c4-e572-483b-98df-c7e335fc7168","_cell_guid":"901fb893-75dc-4a73-a8f4-cf3aff7456df","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**labels (label.csv):**\n\n* patient_id: The unique identifier for each patient.\n* series_id: Identifier for the series of images for the patient.\n* instance_number: The specific image instance number within the series.\n* injury_name: The name or type of injury detected in the image.\n\n**train (train.csv):**\n\nThis dataset provides the labels for different types of injuries for each patient.\nColumns like bowel_healthy, bowel_injury, extravasation_healthy, etc., indicate the health status or injury severity of various organs for each patient.\n\n**train_meta (train_series_meta.csv):**\n\n* patient_id: The unique identifier for each patient.\n* series_id: Identifier for the series of images for the patient.\n* aortic_hu: A quantitative measure related to the images.\n* incomplete_organ: A binary indicator specifying whether the organ is incomplete in the images.","metadata":{"_uuid":"29132157-788d-496d-9a57-17c3ba593e2c","_cell_guid":"dd105b85-e1af-420f-96bb-a7cbc54bdce8","trusted":true}},{"cell_type":"code","source":"merged_df = pd.merge(train, train_meta, on='patient_id', how='outer')","metadata":{"_uuid":"5b776336-8f97-4545-a968-500b7d503a45","_cell_guid":"cbdf0b2c-9ffd-47f7-b30a-75b689aa81e1","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of rows in merged_df:\", len(merged_df))\nprint(\"Number of rows in train:\", len(train))\nprint(\"Number of rows in train_meta:\", len(train_meta))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_df['any_injury'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['any_injury'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_counts = merged_df['any_injury'].value_counts()\nclass_percentages = merged_df['any_injury'].value_counts(normalize=True) * 100\n\nprint(\"Class counts in the training set:\")\nprint(class_counts)\n\nprint(\"\\nClass percentages in the training set:\")\nprint(class_percentages)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_counts = train['any_injury'].value_counts()\nclass_percentages = train['any_injury'].value_counts(normalize=True) * 100\n\nprint(\"Class counts in the training set:\")\nprint(class_counts)\n\nprint(\"\\nClass percentages in the training set:\")\nprint(class_percentages)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"complete_df = pd.merge(merged_df, labels, on='patient_id', how='inner')\ncomplete_df","metadata":{"_uuid":"308a0fd1-109c-45eb-9ff7-b67862198ae6","_cell_guid":"4c05bad8-2724-4cdd-9040-bb9ba3d4e4a3","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_counts = complete_df['any_injury'].value_counts()\nclass_percentages = complete_df['any_injury'].value_counts(normalize=True) * 100\n\nprint(\"Class counts in the training set:\")\nprint(class_counts)\n\nprint(\"\\nClass percentages in the training set:\")\nprint(class_percentages)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_df = complete_df.drop(['patient_id', 'any_injury','series_id_x','series_id_y', 'instance_number', 'injury_name'], axis=1)","metadata":{"_uuid":"b66c8e8a-8c23-4c96-ab5d-dea79ccb2b27","_cell_guid":"fc7f1a1d-d254-4882-b045-d6d2af345377","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlation_matrix = corr_df.corr()\ncorrelation_matrix","metadata":{"_uuid":"4d01b640-5c18-4a70-8f24-fb3e9968834b","_cell_guid":"4b83704f-41b6-4983-b694-5ebfe4cd102c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install seaborn\n\nimport seaborn as sns\nplt.figure(figsize=(10, 8))\n# Create the heatmap\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n\n# Add a title\nplt.title('Correlation Matrix Heatmap')\n\n# Show the plot\nplt.show()","metadata":{"_uuid":"86230d05-2bf2-4318-8f02-cd4535c8ed99","_cell_guid":"687b21f1-0a9a-422b-8c16-5ea873923e54","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlation_with_aortic_hu = corr_df.corr()['aortic_hu']\nplt.figure(figsize=(12, 6))\ncorr_df.corr()['aortic_hu'].plot(kind='bar', color='skyblue')\nplt.xlabel('Columns')\nplt.ylabel('Correlation')\nplt.title('Correlation of \"aortic_hu\" with Other Columns')\nplt.xticks(rotation=90)\nplt.show()","metadata":{"_uuid":"810feb96-5596-416c-9499-f69b780c2965","_cell_guid":"0c39555c-0d38-4821-baf3-5729e1b04889","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Understanding**","metadata":{"_uuid":"b14ae972-4dcf-4360-b1f8-cf388ac8f5ca","_cell_guid":"509a5a72-2e57-408f-b69a-bac1805ab5e5","trusted":true}},{"cell_type":"code","source":"# Basic information for the 'train' dataset\ntrain_info = {\n    \"Number of Rows\": train.shape[0],\n    \"Number of Columns\": train.shape[1],\n    \"Columns\": train.columns.tolist(),\n    \"Data Types\": train.dtypes.tolist(),\n    \"Unique Values per Column\": train.nunique().tolist()\n}\n\n# Basic information for the 'labels' dataset\nlabels_info = {\n    \"Number of Rows\": labels.shape[0],\n    \"Number of Columns\": labels.shape[1],\n    \"Columns\": labels.columns.tolist(),\n    \"Data Types\": labels.dtypes.tolist(),\n    \"Unique Values per Column\": labels.nunique().tolist()\n}\n\n# Basic information for the 'train_meta' dataset\ntrain_meta_info = {\n    \"Number of Rows\": train_meta.shape[0],\n    \"Number of Columns\": train_meta.shape[1],\n    \"Columns\": train_meta.columns.tolist(),\n    \"Data Types\": train_meta.dtypes.tolist(),\n    \"Unique Values per Column\": train_meta.nunique().tolist()\n}\n\ntrain_info, labels_info, train_meta_info","metadata":{"_uuid":"423638c0-8943-4cc1-a917-b70608e2a267","_cell_guid":"20823d64-b7f1-4026-9ff1-8b8a04ea3f2b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**1. labels (image_level_labels.csv) Dataset:**\n\n- Number of Rows: 12,029\n- Number of Columns: 4\n- Columns:\n    - patient_id: Unique dentifier of the patient.\n    - series_id: An identifier for the series of images associated with each patient..\n    - instance_number: Specific image instance number within the series.\n    - injury_name: Type of injury detected in the image.\n- Data Types: The data types are appropriate with integer types for identifiers and object (string) type for the injury name.\n- Unique Values: There are 246 unique patients, 330 unique series, and 925 unique instance numbers. The injury_name column has 2 unique values, indicating two types of injuries;  Active_Extravasation and bowel\n\n**2. train(train.csv) Dataset:**\n\n- Number of Rows: 3,147\n- Number of Columns: 15\n- Columns:\n    - patient_id: Unique identifier of the patient.\n    - The other 14 columns represent the health status and injury severity of various organs for each patient, recorded as binary variables where 0 indicates the absence of a condition, and 1 indicates the presence of a condition.\n- Data Types: All columns are of integer type.\n- Unique Values: There are 3,147 unique patients. The injury-related columns have binary values (0 or 1), indicating the absence or presence of a specific injury type.\n\n**3. train_meta (train_series_meta.csv) Dataset:**\n\n- Number of Rows: 4,711\n- Number of Columns: 4\n- Columns:\n    - patient_id: Unique identifier of the patient.\n    - series_id: An identifier for the series of images associated with each patient..\n    - aortic_hu: A quantitative measure in HU related to the aorta.\n    - incomplete_organ: A binary indicator where 0 signifies the absence of an incomplete organ, and 1 signifies the presence of an incomplete organ..\n- Data Types: The data types are appropriate with integer and float types.\n- Unique Values: There are 3,147 unique patients and 4,711 unique series. The incomplete_organ column has 2 unique values.","metadata":{"_uuid":"fc2c0bf7-362a-4e02-8b76-de1f3f81b390","_cell_guid":"6f1bf329-681f-4fd3-ab26-0eab08285619","trusted":true}},{"cell_type":"code","source":"# Checking for missing values and duplicates\ndef check_missing_and_duplicates(datasets):\n    # Initializing lists to store the results\n    dataset_names = []\n    missing_values_list = []\n    duplicates_list = []\n    \n    for dataset_name, dataset in datasets.items():\n        # Calculating missing values\n        missing_values = dataset.isnull().sum().sum()\n        \n        # Checking for duplicates\n        duplicates = dataset.duplicated().sum()\n        \n        # Appending \n        dataset_names.append(dataset_name)\n        missing_values_list.append(missing_values)\n        duplicates_list.append(duplicates)\n    \n    # Creating a summary DataFrame\n    summary_df = pd.DataFrame({\n        \"Dataset\": dataset_names,\n        \"Missing Values\": missing_values_list,\n        \"Duplicates\": duplicates_list\n    })\n    \n    return summary_df\n\ndatasets = {\n    \"train\": train,\n    \"labels\": labels,\n    \"train_meta\": train_meta\n}\n\nsummary = check_missing_and_duplicates(datasets)\n\nprint(summary)","metadata":{"_uuid":"031e63b1-2254-4b60-9a4c-0036936ccd1e","_cell_guid":"5966d27f-df6f-4251-a92d-5d5b9803378b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- There are no missing values in any of the datasets.\n- There are no duplicated rows in any of the datasets.","metadata":{"_uuid":"e79d58e2-beef-47a0-b3dd-250cb051f040","_cell_guid":"359e9707-730c-4886-81cc-f204415a53d1","trusted":true}},{"cell_type":"code","source":"print(\"Descriptive statistics for the train dataset:\")\nprint(train.describe())","metadata":{"_uuid":"82210c83-cb56-4509-bfbc-65ae7f233212","_cell_guid":"caa0fe9c-1f78-4bee-afaf-b31370352461","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Statistical Summary of the train dataset**\n\n**1. Patient IDs Distribution:**\n\n- Representing the unique patient identifier, the 'patient_id' column ranges from 19 to 65,508, suggesting a wide range of patients in the dataset.\n\n**2. Organ Health Status:**\n\n- Several columns (e.g., 'bowel_healthy', 'extravasation_healthy', 'kidney_healthy', 'liver_healthy', 'spleen_healthy') are binary indicators of organ health.\n- On average, most patients have healthy organs, as indicated by values close to 1.\n- The mean values of these columns are as per below:\n    - The 'bowel_healthy' has an approximate mean of 0.98.\n    - The 'extravasation_healthy' has a 0.94 mean.\n    - The 'kidney_healthy' has 0.94 mean.\n    - The 'liver_healthy'has a 0.90 mean.\n    - The 'spleen_healthy' has a 0.89 mean.\n- This suggests that these organ related injuries are relatively rare in the dataset.\n\n**3. Organ Injury Severity:**\n\n- Columns like 'bowel_injury', 'extravasation_injury', 'kidney_low', 'kidney_high', 'liver_low', 'liver_high', 'spleen_low', and 'spleen_high' represent binary indicators of injury severity for various organs.  The mean values of these columns are as per below:\n    - The 'liver_high'has a mean of around 0.02 while 'liver_low' has a mean of 0.08. \n    - The 'spleen_high' is at 0.05 with 'spleen_low' at 0.06.\n    - The 'kidney_high' is at 0.02 with 'kidney_low' at 0.04.\n    - The 'bowel_injury' is at 0.02 with the 'extravasation_injury' at 0.06.\n- These columns have low mean values, further confirming that severe injuries are relatively uncommon as compared to healthy organs.\n\n**4. Overall Injury Presence:**\n\n- The 'any_injury' column is a binary indicator of the presence of any injury in a patient.\n- On average, approximately 27% of patients in the dataset have at least one injury (mean value of 0.27).\n\n**Conclusions:**\n\n- The dataset appears to be relatively imbalanced, with most patients having healthy organs and a minority experiencing injuries.\n\n- Bowel injuries, extravasation injuries, kidney injuries, liver injuries, and spleen injuries are relatively rare, as indicated by low mean values for their respective columns.\n\n- Most patients have healthy organs, suggesting that the dataset may contain a majority of cases without severe injuries.\n\n**Approximately 27% of patients in the dataset have at least one injury, indicating that injuries, while less common, are still present in a significant portion of the population. This underscores the importance of conducting further EDA to gain deeper insights into the nature, patterns, and potential risk factors associated with these injuries. EDA will help us better understand the characteristics of injuries and their impact on patient outcomes, leading to more informed decision-making in the field of trauma care and intervention.**","metadata":{"_uuid":"bea73e1c-cc42-4960-9154-d561d9dfd3ee","_cell_guid":"af3924ea-b932-48d4-82fc-fa3bf94e9a74","trusted":true}},{"cell_type":"markdown","source":"## **Exploratory data Analysis**\n\n### **Univariate Analysis**","metadata":{"_uuid":"f7b4b572-2dbd-4536-ab81-6cec0ebb1af8","_cell_guid":"c9983fdb-5d37-453e-a053-40e7d88bd9ba","execution":{"iopub.status.busy":"2023-09-21T11:20:23.208756Z","iopub.execute_input":"2023-09-21T11:20:23.209230Z","iopub.status.idle":"2023-09-21T11:20:23.217595Z","shell.execute_reply.started":"2023-09-21T11:20:23.209194Z","shell.execute_reply":"2023-09-21T11:20:23.215839Z"},"trusted":true}},{"cell_type":"code","source":"# Visualizing the distribution of injury types in the 'label' dataset\nplt.figure(figsize=(10, 6))\nsns.countplot(data=labels, x='injury_name')\nplt.title('Distribution of Injury Types in train Dataset')\nplt.ylabel('Count')\nplt.xlabel('Injury Type')\nplt.show()","metadata":{"_uuid":"a8163582-d5db-4150-949e-1e0efd3137a4","_cell_guid":"cf33ca40-5ddb-4b27-92f7-b15029372199","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data suggests that extravasation (active bleeding) is more frequently identified in the provided images than bowel injuries.","metadata":{"_uuid":"ec58cc70-578b-483d-994c-bd2787b75003","_cell_guid":"46a5d1d1-c479-4a1e-a13f-59f4946a8e4a","trusted":true}},{"cell_type":"code","source":"# Visualizing the distribution of injury-related columns in the 'train' dataset\ninjury_columns = [col for col in train.columns if col != \"patient_id\"]\ninjury_counts = train[injury_columns].sum()\n\nplt.figure(figsize=(14, 8))\ninjury_counts.sort_values().plot(kind='barh')\nplt.title('Distribution of Injury-Related Columns in labels Dataset')\nplt.xlabel('Count')\nplt.ylabel('Injury Type / Health Status')\nplt.show()","metadata":{"_uuid":"9fb61afc-7d74-4a14-b614-edf0c41ec722","_cell_guid":"c3306b76-b282-4395-b5a8-5bbca3afe0f9","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing the distribution of the 'aortic_hu' column in the 'train_meta' dataset\nplt.figure(figsize=(10, 6))\nsns.histplot(train_meta['aortic_hu'], bins=50, kde=True)\nplt.title('Distribution of Aortic HU in train_meta Dataset')\nplt.xlabel('Aortic HU')\nplt.ylabel('Count')\nplt.show()","metadata":{"_uuid":"d8d80b50-d12e-4bfb-8847-9ea34b1a627b","_cell_guid":"aeff8548-3d5b-4580-82f6-9bc86cb8614f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"_uuid":"5625bc55-92f9-4c9f-98ad-69a41b686769","_cell_guid":"9bbf423a-5aa2-4861-b2f9-2184913f3e85","trusted":true}},{"cell_type":"markdown","source":"Hounsfield Units (HU) are a measure used in CT scans to describe radiodensity, and the distribution gives us an idea of the variation in these values across different images.","metadata":{"_uuid":"2611a0de-893d-40a4-8ae3-4a2067879d8e","_cell_guid":"403909d2-ae39-4283-b50b-f4c02cc31f25","trusted":true}},{"cell_type":"markdown","source":"**Relationship Analysis:**","metadata":{"_uuid":"ef152e16-45c1-485f-bffd-eaa05f76611e","_cell_guid":"6435bd4b-92a2-4a01-8b15-5745a0df1bf8","trusted":true}},{"cell_type":"code","source":"# Visualizing the relationship between 'aortic_hu' and 'incomplete_organ' in the 'train_meta' dataset\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=train_meta, x='incomplete_organ', y='aortic_hu')\nplt.title('Relationship between Aortic HU and Incomplete Organ in train_meta Dataset')\nplt.xlabel('Incomplete Organ (0 = Complete, 1 = Incomplete)')\nplt.ylabel('Aortic HU')\nplt.show()","metadata":{"_uuid":"1f11beee-6756-4b4c-82e7-c06b2616ed0b","_cell_guid":"1fd537ed-25e3-41da-afbc-447886437183","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This suggests that there might be some relationship between the completeness of the organ in the image and the aortic_hu values.","metadata":{"_uuid":"fc0726fc-8b46-4b5f-9e8d-3198a86f4ab9","_cell_guid":"e987beaf-ee5e-41fe-9199-137c85341de9","trusted":true}},{"cell_type":"markdown","source":"**Outliers Analysis:**","metadata":{"_uuid":"15494f72-cac2-44da-aa55-f830bbe78a0c","_cell_guid":"c9336423-97f6-491c-8fd8-e41be76d8380","trusted":true}},{"cell_type":"code","source":"# Outlier analysis for the 'aortic_hu' column using the IQR method\n\n# Calculate Q1, Q3, and IQR\nQ1 = train_meta['aortic_hu'].quantile(0.25)\nQ3 = train_meta['aortic_hu'].quantile(0.75)\nIQR = Q3 - Q1\n\n# Define bounds for outliers\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\n# Identify outliers\noutliers = train_meta[(train_meta['aortic_hu'] < lower_bound) | (train_meta['aortic_hu'] > upper_bound)]\n\n# Percentage of data points that are outliers\noutlier_percentage = (len(outliers) / len(train_meta)) * 100\n\noutlier_summary = {\n    \"Lower Bound\": lower_bound,\n    \"Upper Bound\": upper_bound,\n    \"Number of Outliers\": len(outliers),\n    \"Percentage of Outliers\": outlier_percentage\n}\n\noutlier_summary","metadata":{"_uuid":"120023d5-69c0-4301-b744-3fd8c2b92732","_cell_guid":"5a63c59f-3a8d-4153-b4f4-c2b5589a0f03","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing outliers for the 'aortic_hu' column\nplt.figure(figsize=(12, 8))\nsns.boxplot(train_meta['aortic_hu'])\nplt.axhline(lower_bound, color='r', linestyle='--', label=f\"Lower Bound: {lower_bound}\")\nplt.axhline(upper_bound, color='g', linestyle='--', label=f\"Upper Bound: {upper_bound}\")\nplt.title('Boxplot of Aortic HU with Outliers Highlighted')\nplt.xlabel('Aortic HU')\nplt.legend()\nplt.show()","metadata":{"_uuid":"c2bf69d2-cf74-45cd-8b99-1e5732d26aa9","_cell_guid":"44798b9b-40be-4464-b16a-fc4cf01431b8","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the plot, we can observe a cluster of data points above the upper bound, indicating potential outliers with higher aortic_hu values.","metadata":{"_uuid":"e7756723-955e-4ef9-bd7f-a2f89c4b580a","_cell_guid":"58fd8705-459b-4694-8108-29f724cce3eb","trusted":true}},{"cell_type":"markdown","source":"**Relationship Analysis:**","metadata":{"_uuid":"bba62ba8-ccd0-4201-a6ab-c2b2240a1d80","_cell_guid":"e3fd9fce-b6b4-4ba5-be8f-236bd559fe31","trusted":true}},{"cell_type":"markdown","source":"**1. Injury Type vs. Aortic HU:**","metadata":{"_uuid":"2ba638d4-7405-4935-b759-d2217c83266c","_cell_guid":"c64c83fe-9924-4284-9a5f-e20e6cd322d4","trusted":true}},{"cell_type":"code","source":"# Merging the 'label' and 'train_meta' datasets on 'patient_id' and 'series_id'\nmerged_data = pd.merge(labels, train_meta, on=['patient_id', 'series_id'])\n\n# Visualizing the distribution of 'aortic_hu' based on 'injury_name'\nplt.figure(figsize=(12, 8))\nsns.boxplot(data=merged_data, x='injury_name', y='aortic_hu')\nplt.title('Distribution of Aortic HU based on Injury Type')\nplt.xlabel('Injury Type')\nplt.ylabel('Aortic HU')\nplt.show()","metadata":{"_uuid":"ff007837-cb2f-4559-9ab6-edcd015bbf89","_cell_guid":"0f487548-721e-411e-9ea0-1deff37d1da8","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For bowel_injury, the distribution appears to have a slightly higher median and is more compact in terms of the interquartile range (IQR) compared to extravasation.\nThe extravasation injury (which represents active bleeding) has a broader IQR, indicating more variability in the aortic_hu values for this injury type. There are also a few potential outliers present for this injury type.","metadata":{"_uuid":"cd10d8fc-1140-4133-96e5-8325050b46d2","_cell_guid":"df412913-8ece-4882-a583-58e72892a371","trusted":true}},{"cell_type":"markdown","source":"**2. Injury Type vs. Completeness of Organ:**","metadata":{"_uuid":"6f052404-b26b-447b-8650-e133a8bc08c2","_cell_guid":"a92498de-e06e-43c2-aa2b-8eeeefca0069","trusted":true}},{"cell_type":"code","source":"# Visualizing the relationship between 'injury_name' and 'incomplete_organ'\nplt.figure(figsize=(10, 6))\nsns.countplot(data=merged_data, x='injury_name', hue='incomplete_organ')\nplt.title('Injury Type vs. Completeness of Organ')\nplt.xlabel('Injury Type')\nplt.ylabel('Count')\nplt.legend(title='Incomplete Organ (0 = Complete, 1 = Incomplete)')\nplt.show()","metadata":{"_uuid":"f55b52e8-3696-4724-a398-ae4e9ff114d7","_cell_guid":"03448a94-7e64-446c-b5f1-1c81013892f0","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For both bowel_injury and extravasation injury types, the majority of the organs in the images are complete (incomplete_organ = 0).\nThe number of images with incomplete organs (incomplete_organ = 1) is relatively lower for both injury types, with extravasation having a slightly higher count of incomplete organs compared to bowel_injury.","metadata":{"_uuid":"b747a93c-4f28-4eb9-bfc8-ba559491db41","_cell_guid":"7cf221dd-d265-4efe-baec-1e2b38b51580","trusted":true}},{"cell_type":"markdown","source":"## Importing Images","metadata":{"_uuid":"2b4dc373-6952-48b6-81dc-27c0f1d4c6b3","_cell_guid":"9da7eddd-a5f0-4023-a8f2-846208d7db5d","trusted":true}},{"cell_type":"markdown","source":"Start by creating image paths for test dataset","metadata":{"_uuid":"230fbbef-3bfa-4ce1-9d2d-044b18e2271c","_cell_guid":"1b2fed75-3918-4ff1-a486-a0e8e4f5d541","trusted":true}},{"cell_type":"code","source":"# Adjusting the path generation function to exclude instance_number\ndef test_img_path(row):\n    return f\"/kaggle/input/rsna-2023-abdominal-trauma-detection/test_images/{row['patient_id']}/{row['series_id']}/\"\n\ntest_meta['test_img_path'] = test_meta.apply(test_img_path, axis=1)\n\n# Display the first few rows of the test_meta dataframe with the new 'adjusted_img_path' column\ntest_meta.head()","metadata":{"_uuid":"adaff152-31d2-4061-a7a5-38f50b277a98","_cell_guid":"d0e0de97-c59b-43dd-9214-cba235f7de8c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Craeting image path for train dataset","metadata":{"_uuid":"5a195148-0e73-48fa-a42d-048b1bb24fb7","_cell_guid":"bdbc3cfb-e2dd-4352-b8b3-7715ff65d590","trusted":true}},{"cell_type":"code","source":"def img_path(row):\n    return f\"/kaggle/input/rsna-2023-abdominal-trauma-detection/train_images/{row['patient_id']}/{row['series_id']}/{row['instance_number']}.dcm\"\n\nlabels['img_path'] = labels.apply(img_path, axis=1)","metadata":{"_uuid":"c082d0f1-3fc0-4aa1-adc8-aa8f0c5cacf7","_cell_guid":"6dcb55d9-61ca-4440-b39e-8fe414a82913","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**DICOM Image Visualization:**","metadata":{"_uuid":"f82610f3-fcf5-4e10-87c7-18bffc190183","_cell_guid":"5fa8e64c-645a-4a3e-a73d-0979ae31611a","trusted":true}},{"cell_type":"code","source":"# Generating Kaggle reference paths for the 'train' dataset again\nlabels['img_path'] = labels.apply(img_path, axis=1)\n\n# Displaying the first few rows of the 'train' dataset with the updated 'img_path' column\nlabels.head()","metadata":{"_uuid":"27ae19ca-a981-40b1-baed-1951e9f47e58","_cell_guid":"0bb616b3-6023-4275-8723-33c9751b4753","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def is_valid_image(img_path):\n    return os.path.exists(img_path)\n# Assuming your DataFrame is named 'labels'\nlabels = labels[labels['img_path'].apply(is_valid_image)]\nlabels.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ramdomly display injury type and image","metadata":{"_uuid":"6cf87906-481a-4011-9d9e-dbe2c329709d","_cell_guid":"b01d722c-a20a-4ea9-b394-e0589a4d1103","trusted":true}},{"cell_type":"code","source":"!pip install pydicom\n\nimport pydicom\nimport matplotlib.pyplot as plt\n\ndef read_dicom_image(path):\n    \"\"\"\n    Reads a DICOM image and returns its pixel array.\n    \"\"\"\n    dicom_img = pydicom.dcmread(path)\n    return dicom_img.pixel_array\n\n# Sample 20 rows from the train dataset\nsample_data = labels.sample(20)\n\n# Extract the img_paths and corresponding injury names for labeling\nsample_img_paths = sample_data['img_path'].tolist()\nsample_labels = sample_data['injury_name'].tolist()\n\n# Set up the figure for visualization\nplt.figure(figsize=(15, 30))\n\n# Loop through the sampled image paths and display them in rows of 3 with labels\nfor idx, (img_path, label) in enumerate(zip(sample_img_paths, sample_labels), start=1):\n    plt.subplot(7, 3, idx)  # 7 rows, 3 columns\n    plt.imshow(read_dicom_image(img_path), cmap='gray')\n    plt.title(label)\n    plt.axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"d8e640ad-436f-40b4-896b-ff535e15093b","_cell_guid":"dc8b8fc4-40b2-491a-b34e-df15c6185377","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**comparison of images for each injury type**","metadata":{"_uuid":"24223b9f-662e-4a08-b855-f693bbb0f7c4","_cell_guid":"ffe3f583-c283-4995-a115-3d1001aca7c0","trusted":true}},{"cell_type":"code","source":"import pydicom\nimport matplotlib.pyplot as plt\n\ndef read_dicom_image(path):\n    \"\"\"\n    Reads a DICOM image and returns its pixel array.\n    \"\"\"\n    dicom_img = pydicom.dcmread(path)\n    return dicom_img.pixel_array\n\n# Sample one image path for each injury type\nsample_img_paths = labels.groupby('injury_name').apply(lambda x: x.sample(1)['img_path'].values[0])\nsample_labels = sample_img_paths.index.tolist()\n\n# Set up the figure for visualization\nplt.figure(figsize=(15, 5))\n\n# Loop through the sampled image paths and display them side by side with labels\nfor idx, (img_path, label) in enumerate(zip(sample_img_paths, sample_labels), start=1):\n    plt.subplot(1, len(sample_img_paths), idx)\n    plt.imshow(read_dicom_image(img_path), cmap='gray')\n    plt.title(label)\n    plt.axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"d094eaeb-f5f7-4a4b-9190-2eb921ee59eb","_cell_guid":"84fde876-b399-4255-90b0-0c20074b7d91","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Randomly Display Images by Patient ID","metadata":{"_uuid":"7ca7d420-b75e-462e-8151-b5f4d409e132","_cell_guid":"bb4eda34-6b30-44ca-9099-649331a61672","trusted":true}},{"cell_type":"code","source":"import pydicom\nimport matplotlib.pyplot as plt\nimport random\n\ndef read_dicom_image(path):\n    \"\"\"\n    Reads a DICOM image and returns its pixel array.\n    \"\"\"\n    dicom_img = pydicom.dcmread(path)\n    return dicom_img.pixel_array\n\n# Get unique patient IDs from your DataFrame\nunique_patient_ids = labels['patient_id'].unique()\n\n# Randomly select 5 patient IDs (or you can select a fixed set)\nrandom_patient_ids = random.sample(list(unique_patient_ids), 5)\n\n# Set up a grid for displaying images\nnum_rows = 5  # Number of rows in the grid (one row per patient)\nnum_cols = 5  # Number of columns in the grid (up to 5 images per patient)\nplt.figure(figsize=(15, 10))\n\n# Iterate through randomly selected patient IDs\nfor row, random_patient_id in enumerate(random_patient_ids, start=1):\n    # Filter the DataFrame to get all images for the randomly selected patient\n    patient_images = labels[labels['patient_id'] == random_patient_id]\n    \n    # Get unique series IDs for the patient\n    unique_series_ids = patient_images['series_id'].unique()\n    \n    # Randomly select up to 5 unique series IDs (you can adjust the number)\n    random_series_ids = random.sample(list(unique_series_ids), min(5, len(unique_series_ids)))\n    \n    # Iterate through randomly selected series IDs for the patient\n    for col, random_series_id in enumerate(random_series_ids, start=1):\n        # Filter the DataFrame to get all images for the selected series\n        series_images = patient_images[patient_images['series_id'] == random_series_id]\n        \n        # Display each image in the series\n        for i, (_, image_row) in enumerate(series_images.iterrows(), start=1):\n            image_path = image_row['img_path']\n            plt.subplot(num_rows, num_cols, (row - 1) * num_cols + col)\n            plt.imshow(read_dicom_image(image_path), cmap='gray')\n            plt.title(f'Patient ID: {random_patient_id}\\nSeries ID: {random_series_id}\\nImage {i}')\n            plt.axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"7f7adf32-f988-4c60-8452-d7439371aef5","_cell_guid":"a0949bdb-78ac-4609-9f8f-756140a3092f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**preprocessing :**\n\n\n* Rescaling: Adjusting the intensity values to a standard scale, e.g., between 0 and 1.\n* Resizing: Making sure all images have the same size, especially if they are being fed into a neural network.\n* Histogram Equalization: Enhancing the contrast of images.\n* Normalization: Removing the mean and scaling to unit variance.\n* Data Augmentation: Techniques such as rotation, zooming, and flipping to artificially increase the size of the dataset (useful for training deep learning models).\n* Smoothing\n*Padding","metadata":{"_uuid":"8ad7f65b-5c97-4a1a-b06f-c5e99674bc84","_cell_guid":"7eec1450-2d51-435b-8af0-53b1b20da5e4","trusted":true}},{"cell_type":"code","source":"import pydicom\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load a sample DICOM image\nsample_path = labels['img_path'].iloc[0]\ndicom_img = pydicom.dcmread(sample_path).pixel_array\n\n# Rescale the image to the range [0, 1]\nrescaled_img = cv2.normalize(dicom_img, None, 0, 1, cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n\n# Apply histogram equalization\nequalized_img = cv2.equalizeHist((rescaled_img * 255).astype(np.uint8))\n\n# Plot original and preprocessed images side by side\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.imshow(dicom_img, cmap='gray')\nplt.title('Original Image')\nplt.subplot(1, 2, 2)\nplt.imshow(equalized_img, cmap='gray')\nplt.title('Preprocessed Image')\nplt.show()","metadata":{"_uuid":"1fc60d2c-5dca-45cc-9122-656b43f2e281","_cell_guid":"f4eac7eb-d9a9-4ac9-b3ab-9da382880a2b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pydicom\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Function to read a DICOM image and return its pixel array\ndef read_dicom_image(path):\n    dicom_img = pydicom.dcmread(path)\n    return dicom_img.pixel_array\n\n# Load a sample DICOM image\nsample_path = labels['img_path'].iloc[0]\ndicom_img = read_dicom_image(sample_path)\n\n# Rescale the image to the range [0, 1]\nrescaled_img = cv2.normalize(dicom_img, None, 0, 1, cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n\n# Apply histogram equalization\nequalized_img = cv2.equalizeHist((rescaled_img * 255).astype(np.uint8))\n\n# Apply Gaussian smoothing\nk_size = (5, 5)  # Kernel size for Gaussian filter\nsigma = 0.5      # Standard deviation for Gaussian filter\nsmoothed_img = cv2.GaussianBlur(equalized_img, k_size, sigma)\n\n# Define padding size (top, bottom, left, right)\npadding_size = (20, 20, 20, 20)\n\n# Apply zero-padding\npadded_img = np.pad(smoothed_img, ((padding_size[0], padding_size[1]), (padding_size[2], padding_size[3])), mode='constant', constant_values=0)\n\n# Plot original, rescaled, equalized, smoothed, and padded images\nplt.figure(figsize=(20, 8))\nplt.subplot(1, 5, 1)\nplt.imshow(dicom_img, cmap='gray')\nplt.title('Original Image')\n\nplt.subplot(1, 5, 2)\nplt.imshow(rescaled_img, cmap='gray')\nplt.title('Rescaled Image')\n\nplt.subplot(1, 5, 3)\nplt.imshow(equalized_img, cmap='gray')\nplt.title('Equalized Image')\n\nplt.subplot(1, 5, 4)\nplt.imshow(smoothed_img, cmap='gray')\nplt.title('Smoothed Image')\n\nplt.subplot(1, 5, 5)\nplt.imshow(padded_img, cmap='gray')\nplt.title('Padded Image')\n\nplt.show()","metadata":{"_uuid":"42e18ee9-b63f-4b60-9065-6ba2f9df7570","_cell_guid":"9540ffa5-1d94-4843-89c1-f48f39aeeee7","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pydicom\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport gc\n\ndef read_dicom_image(path):\n    dicom_img = pydicom.dcmread(path)\n    return dicom_img.pixel_array\n\ndef process_image(img):\n    rescaled_img = cv2.normalize(img, None, 0, 1, cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n    equalized_img = cv2.equalizeHist((rescaled_img * 255).astype(np.uint8))\n    k_size = (5, 5)\n    sigma = 0.5\n    smoothed_img = cv2.GaussianBlur(equalized_img, k_size, sigma)\n    padding_size = (20, 20, 20, 20)\n    padded_img = np.pad(smoothed_img, ((padding_size[0], padding_size[1]), (padding_size[2], padding_size[3])), mode='constant', constant_values=0)\n    \n    # Resize to a fixed size\n    resized_img = cv2.resize(padded_img, (256, 256))\n    \n    return resized_img / 255.0  # normalize to [0,1]\n\ndef process_batch(batch):\n    batch_images = []\n    for index, row in batch.iterrows():\n        img = read_dicom_image(row['img_path'])\n        processed_img = process_image(img)\n        batch_images.append(processed_img)\n    return np.stack(batch_images)\n\ndef image_generator(labels_df, batch_size):\n    num_samples = len(labels_df)\n    \n    while True:\n        for start in range(0, num_samples, batch_size):\n            end = min(start + batch_size, num_samples)\n            batch = labels_df.iloc[start:end]\n            batch_images = process_batch(batch)\n            \n            yield batch_images\n            \n            # Free up memory\n            del batch_images\n            gc.collect()\n\n# Sample usage\nbatch_size = 100\ndata_gen = image_generator(labels, batch_size=batch_size)\n\n# To get the next batch of images:\n# next_batch = next(data_gen)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels.head()","metadata":{"_uuid":"16babf0e-3751-4f2f-b484-348b582e5b0d","_cell_guid":"bcb1c2b0-b990-476c-ae7a-3ae39d913bc6","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Modelling**","metadata":{"_uuid":"5f0be13b-c3e5-4279-803e-efdc417dd210","_cell_guid":"732a5d52-b083-4a89-b769-1378b48ad72a","trusted":true}},{"cell_type":"markdown","source":"**Steps for Model Building:**\n* Data Preparation: Split the data into training and validation sets.\n* Data Augmentation: Use data augmentation techniques to artificially increase the size of the training dataset.\n* Model Architecture: Define the CNN architecture.\n* Model Compilation: Specify the loss function, optimizer, and metrics.\n* Model Training: Train the model using the training data.\n* Model Evaluation: Evaluate the model's performance on the validation data.","metadata":{"_uuid":"7fe640db-f07f-4c9a-81a5-4ff2aeed87b6","_cell_guid":"f94bc5e9-50e9-43b8-8990-23b9fbdb6383","trusted":true}},{"cell_type":"markdown","source":"**Model 2\nData Augmentataion**","metadata":{}},{"cell_type":"code","source":"\nmodel2_df = pd.merge(labels,train, on='patient_id', how='inner')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"**Model spliting**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Define column names for labels\ny_train_columns = [\"bowel_injury\", \"extravasation_injury\",\n                   \"kidney_healthy\", \"kidney_low\", \"kidney_high\",\n                   \"liver_healthy\", \"liver_low\", \"liver_high\",\n                   \"spleen_healthy\", \"spleen_low\", \"spleen_high\"]\n\ndef batch_generator(data_df, batch_size):\n    while True:\n        for start in range(0, len(data_df), batch_size):\n            end = min(start + batch_size, len(data_df))\n            batch_df = data_df.iloc[start:end]\n            batch_images = process_batch(batch_df)\n            batch_labels = batch_df[y_train_columns].values\n            yield batch_images, batch_labels\n\n# Splitting the data into training (80%) and validation (20%) sets by patient IDs\ntrain_patient_ids, val_patient_ids = train_test_split(model2_df['patient_id'].unique(), \n                                                      test_size=0.2, \n                                                      random_state=42)\n\n# Use boolean indexing to filter rows in model_df based on patient IDs\ntrain_df = model2_df[model2_df['patient_id'].isin(train_patient_ids)]\nval_df = model2_df[model2_df['patient_id'].isin(val_patient_ids)]\n\n# Create generators for training and validation\n#batch_size = 32\n#train_gen = batch_generator(train_df, batch_size=batch_size)\n#val_gen = batch_generator(val_df, batch_size=batch_size)\n\n# Number of steps per epoch\n#train_steps_per_epoch = len(train_df) // batch_size\n#val_steps_per_epoch = len(val_df) // batch_size\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Checking for class balance**","metadata":{}},{"cell_type":"markdown","source":"# Checking class balance for each individual label in the training set\ntrain_class_balance_readable = {}\n\nfor column in y_train_columns:\n    train_class_balance_readable[column] = train_df[column].value_counts().to_dict()\n\nprint(\"Class balance in training set:\")\nprint(train_class_balance_readable)\n\n# Checking class balance for each individual label in the validation set\nval_class_balance_readable = {}\n\nfor column in y_train_columns:\n    val_class_balance_readable[column] = val_df[column].value_counts().to_dict()\n\nprint(\"\\nClass balance in validation set:\")\nprint(val_class_balance_readable)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-09T07:13:36.038546Z","iopub.execute_input":"2023-10-09T07:13:36.039611Z","iopub.status.idle":"2023-10-09T07:13:36.069065Z","shell.execute_reply.started":"2023-10-09T07:13:36.039568Z","shell.execute_reply":"2023-10-09T07:13:36.068163Z"}}},{"cell_type":"markdown","source":"**Upsampling minority class**","metadata":{}},{"cell_type":"code","source":"from sklearn.utils import resample\n\n# Identify majority and minority class combinations\nclass_combinations_counts = train_df[y_train_columns].value_counts()\nmajority_class_combination = class_combinations_counts.idxmax()\nminority_class_combinations = class_combinations_counts.index[class_combinations_counts != class_combinations_counts.max()].tolist()\n\ndf_majority = train_df[train_df[y_train_columns].apply(tuple, axis=1) == majority_class_combination]\ndfs_upsampled = [df_majority]\n\n# Upsample each minority class combination\nfor minority_class_combination in minority_class_combinations:\n    condition = (train_df[y_train_columns].apply(tuple, axis=1) == minority_class_combination)\n    df_minority = train_df[condition]\n    \n    df_minority_upsampled = resample(df_minority, \n                                     replace=True, \n                                     n_samples=len(df_majority), \n                                     random_state=42)\n    \n    dfs_upsampled.append(df_minority_upsampled)\n\n# Combine the majority class with the upsampled minority classes\ndf_upsampled = pd.concat(dfs_upsampled)\n\n# Shuffle the dataframe to mix the data\ntrain_df = df_upsampled.sample(frac=1, random_state=42).reset_index(drop=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**confirming class balance**","metadata":{}},{"cell_type":"code","source":"# Checking class balance for each individual label in the upsampled training set\nclass_balance_readable = {}\n\nfor column in y_train_columns:\n    class_balance_readable[column] = train_df[column].value_counts().to_dict()\n\nclass_balance_readable\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check class balance in the validation set\nclass_balance_val = val_df[y_train_columns].sum().to_dict()\nclass_balance_val\n","metadata":{"execution":{"iopub.status.busy":"2023-10-09T07:13:38.268622Z","iopub.execute_input":"2023-10-09T07:13:38.268974Z","iopub.status.idle":"2023-10-09T07:13:38.291692Z","shell.execute_reply.started":"2023-10-09T07:13:38.268946Z","shell.execute_reply":"2023-10-09T07:13:38.290208Z"}}},{"cell_type":"markdown","source":"**IMage augmentation and preprosssing**","metadata":{}},{"cell_type":"code","source":"import pydicom\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport gc\n\n# Read DICOM image from a file path\ndef read_dicom_image(path):\n    dicom_img = pydicom.dcmread(path)\n    return dicom_img.pixel_array\n\n# Augmentation\ndef augment_image(img):\n    # Random rotation\n    angle = np.random.randint(-15, 15)\n    M = cv2.getRotationMatrix2D((img.shape[1] // 2, img.shape[0] // 2), angle, 1)\n    img = cv2.warpAffine(img, M, (img.shape[1], img.shape[0]))\n\n    # Random flip\n    if np.random.rand() > 0.5:\n        img = cv2.flip(img, 1)  # horizontal flip\n\n    return img\n\n# Process the image\ndef process_image(img):\n    rescaled_img = cv2.normalize(img, None, 0, 1, cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n    equalized_img = cv2.equalizeHist((rescaled_img * 255).astype(np.uint8))\n    k_size = (5, 5)\n    sigma = 0.5\n    smoothed_img = cv2.GaussianBlur(equalized_img, k_size, sigma)\n    padding_size = (20, 20, 20, 20)\n    padded_img = np.pad(smoothed_img, ((padding_size[0], padding_size[1]), (padding_size[2], padding_size[3])), mode='constant', constant_values=0)\n    \n    # Resize to a fixed size\n    resized_img = cv2.resize(padded_img, (256, 256))\n    \n    return resized_img / 255.0  # normalize to [0,1]\n\n# Process a batch of images\ndef process_batch(batch):\n    batch_images = []\n    for index, row in batch.iterrows():\n        img = read_dicom_image(row['img_path'])\n        processed_img = process_image(img)\n        batch_images.append(processed_img)\n    return np.stack(batch_images)\n\ndef image_generator(labels_df, batch_size):\n    num_samples = len(labels_df)\n    \n    while True:\n        for start in range(0, num_samples, batch_size):\n            end = min(start + batch_size, num_samples)\n            batch = labels_df.iloc[start:end]\n            batch_images = process_batch(batch)\n            \n            # Define labels for each task\n            labels_bowel = batch['bowel_injury'].values  # Binary classification\n            labels_extravasation = batch['extravasation_injury'].values  # Binary classification\n            \n            labels_kidney = batch[['kidney_healthy', 'kidney_low', 'kidney_high']].values  # Multi-class (3 classes)\n            labels_liver = batch[['liver_healthy', 'liver_low', 'liver_high']].values  # Multi-class (3 classes)\n            labels_spleen = batch[['spleen_healthy', 'spleen_low', 'spleen_high']].values  # Multi-class (3 classes)\n            \n            # Yield the batch of images and labels\n            yield batch_images, [labels_bowel, labels_extravasation, labels_kidney, labels_liver, labels_spleen]\n            \n            # Free up memory\n            del batch_images\n            gc.collect()\n\n# Sample usage\nbatch_size = 100\ntrain_gen = image_generator(train_df, batch_size=batch_size)\nval_gen = image_generator(val_df, batch_size=batch_size)\n\n# To get the next batch of images and labels for training:\nX_train_batch, y_train_batch = next(train_gen)\n\n# And for validation:\nX_val_batch, y_val_batch = next(val_gen)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\n\n# Mocking some values for config\nclass Config:\n    IMAGE_SIZE = (256, 256)\n    BATCH_SIZE = 200\n\nconfig = Config()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\n\n# Mocking some values for config\nclass Config:\n    IMAGE_SIZE = (256, 256)\n    BATCH_SIZE = 200\n\nconfig = Config()\n\n# 1. Detect and initialize the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n# 2. Instantiate a distribution strategy\ntpu_strategy = tf.distribute.TPUStrategy(tpu)\n\n# 3. Define a function to create the model within the strategy scope\ndef create_model():\n    # Define Input\n    inputs = keras.Input(shape=config.IMAGE_SIZE + (1,), batch_size=config.BATCH_SIZE)  # Use a tuple instead of a list\n\n    # Define Backbone\n    # Using standard ResNet50 from keras\n    backbone = keras.applications.ResNet50(include_top=False, weights=None, input_tensor=inputs)\n    x = backbone.output\n\n    # GAP to get the activation maps\n    gap = keras.layers.GlobalAveragePooling2D()\n    x = gap(x)\n\n    # Adding dropout for regularization\n    x = keras.layers.Dropout(0.5)(x)\n\n    # Define 'necks' for each head\n    x_bowel = keras.layers.Dense(32, activation='relu')(x)\n    x_extra = keras.layers.Dense(32, activation='relu')(x)\n    x_liver = keras.layers.Dense(32, activation='relu')(x)\n    x_kidney = keras.layers.Dense(32, activation='relu')(x)\n    x_spleen = keras.layers.Dense(32, activation='relu')(x)\n\n    # Define heads\n    out_bowel = keras.layers.Dense(1, name='bowel', activation='sigmoid')(x_bowel)\n    out_extra = keras.layers.Dense(1, name='extra', activation='sigmoid')(x_extra)\n    out_liver = keras.layers.Dense(3, name='liver', activation='softmax')(x_liver)\n    out_kidney = keras.layers.Dense(3, name='kidney', activation='softmax')(x_kidney)\n    out_spleen = keras.layers.Dense(3, name='spleen', activation='softmax')(x_spleen)\n\n    # Concatenate the outputs\n    outputs = [out_bowel, out_extra, out_liver, out_kidney, out_spleen]\n\n    return keras.Model(inputs=inputs, outputs=outputs)\n\n# 4. Create the model within the strategy scope\nwith tpu_strategy.scope():\n    model = create_model()\n\n    # Cosine Decay\n    cosine_decay = keras.optimizers.schedules.CosineDecay(\n        initial_learning_rate=1e-4,\n        decay_steps=decay_steps,\n        alpha=0.0\n    )\n\n    # Compile the model\n    optimizer = keras.optimizers.Adam(learning_rate=cosine_decay)\n    loss = {\n        \"bowel\": keras.losses.BinaryCrossentropy(),\n        \"extra\": keras.losses.BinaryCrossentropy(),\n        \"liver\": keras.losses.CategoricalCrossentropy(),\n        \"kidney\": keras.losses.CategoricalCrossentropy(),\n        \"spleen\": keras.losses.CategoricalCrossentropy(),\n    }\n    metrics = {\n        \"bowel\": [\"accuracy\"],\n        \"extra\": [\"accuracy\"],\n        \"liver\": [\"accuracy\"],\n        \"kidney\": [\"accuracy\"],\n        \"spleen\": [\"accuracy\"],\n    }\n\n    # Compile the model within the strategy scope\n    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n\n# 5. Print the model summary\nmodel.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modified model-building code\n# Mocking some values needed for training\n# These are just placeholders since we don't have the actual values in this environment.\nwarmup_steps = 10\ndecay_steps = 100\n# Define Input\n# Define Input\ninputs = keras.Input(shape=config.IMAGE_SIZE + (1,), batch_size=config.BATCH_SIZE)  # Use a tuple instead of a list\n\n# Define Backbone\n# Using standard ResNet50 from keras\nbackbone = keras.applications.ResNet50(include_top=False, weights=None, input_tensor=inputs)\nx = backbone.output\n\n# GAP to get the activation maps\ngap = keras.layers.GlobalAveragePooling2D()\nx = gap(x)\n\n# Adding dropout for regularization\nx = keras.layers.Dropout(0.5)(x)\n\n# Define 'necks' for each head\nx_bowel = keras.layers.Dense(32, activation='relu')(x)\nx_extra = keras.layers.Dense(32, activation='relu')(x)\nx_liver = keras.layers.Dense(32, activation='relu')(x)\nx_kidney = keras.layers.Dense(32, activation='relu')(x)\nx_spleen = keras.layers.Dense(32, activation='relu')(x)\n\n# Define heads\nout_bowel = keras.layers.Dense(1, name='bowel', activation='sigmoid')(x_bowel)\nout_extra = keras.layers.Dense(1, name='extra', activation='sigmoid')(x_extra)\nout_liver = keras.layers.Dense(3, name='liver', activation='softmax')(x_liver)\nout_kidney = keras.layers.Dense(3, name='kidney', activation='softmax')(x_kidney)\nout_spleen = keras.layers.Dense(3, name='spleen', activation='softmax')(x_spleen)\n\n# Concatenate the outputs\noutputs = [out_bowel, out_extra, out_liver, out_kidney, out_spleen]\n\n# Create model\nprint(\"[INFO] Building the model...\")\nmodel = keras.Model(inputs=inputs, outputs=outputs)\n\n# Cosine Decay\ncosine_decay = keras.optimizers.schedules.CosineDecay(\n    initial_learning_rate=1e-4,\n    decay_steps=decay_steps,\n    alpha=0.0\n)\n\n # Compile the model\noptimizer = keras.optimizers.Adam(learning_rate=cosine_decay)\nloss = {\n        \"bowel\":keras.losses.BinaryCrossentropy(),\n        \"extra\":keras.losses.BinaryCrossentropy(),\n        \"liver\":keras.losses.CategoricalCrossentropy(),\n        \"kidney\":keras.losses.CategoricalCrossentropy(),\n        \"spleen\":keras.losses.CategoricalCrossentropy(),\n    }\nmetrics = {\n        \"bowel\":[\"accuracy\"],\n        \"extra\":[\"accuracy\"],\n        \"liver\":[\"accuracy\"],\n        \"kidney\":[\"accuracy\"],\n        \"spleen\":[\"accuracy\"],\n    }\nprint(\"[INFO] Compiling the model...\")\n# Consider adding 'loss_weights' if needed\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-10T05:45:22.913693Z","iopub.execute_input":"2023-10-10T05:45:22.913960Z","iopub.status.idle":"2023-10-10T05:45:28.215335Z","shell.execute_reply.started":"2023-10-10T05:45:22.913935Z","shell.execute_reply":"2023-10-10T05:45:28.214119Z"}}},{"cell_type":"code","source":"# Mocking some values needed for training\n# These are just placeholders since we don't have the actual values in this environment.\nwarmup_steps = 10\ndecay_steps = 100\n\n# 1. Set Parameters\nepochs = 10\nsteps_per_epoch = len(train_df) // 300\nvalidation_steps = len(val_df) // 300\n\n# 2. (Optional) Callbacks\n# For demonstration, we'll use the ModelCheckpoint callback to save the best model.\ncheckpoint_callback = keras.callbacks.ModelCheckpoint(\n    'best_model.h5', \n    monitor='val_loss', \n    verbose=1, \n    save_best_only=True, \n    mode='min'\n)\n\n# 3. Train the Model\n# Note: We're using 'fit' instead of 'fit_generator' as 'fit' can handle generators in TensorFlow 2.x.\n# This is a mock code and won't run in this environment without the necessary libraries and data.\n\nhistory = model.fit(\n    train_gen,\n    steps_per_epoch=steps_per_epoch,\n    epochs=epochs,\n    validation_data=val_gen,\n    validation_steps=validation_steps,\n    callbacks=[checkpoint_callback]\n)\n\n\n# The code in triple quotes is what you'd execute in your environment.\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Visualize the training plots**","metadata":{}},{"cell_type":"code","source":"# Create a 3x2 grid for the subplots\nfig, axes = plt.subplots(5, 1, figsize=(5, 15))\n\n# Flatten axes to iterate through them\naxes = axes.flatten()\n\n# Iterate through the metrics and plot them\nfor i, name in enumerate([\"bowel\", \"extra\", \"kidney\", \"liver\", \"spleen\"]):\n    # Plot training accuracy\n    axes[i].plot(history.history[name + '_accuracy'], label='Training ' + name)\n    # Plot validation accuracy\n    axes[i].plot(history.history['val_' + name + '_accuracy'], label='Validation ' + name)\n    axes[i].set_title(name)\n    axes[i].set_xlabel('Epoch')\n    axes[i].set_ylabel('Accuracy')\n    axes[i].legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history[\"loss\"], label=\"loss\")\nplt.plot(history.history[\"val_loss\"], label=\"val loss\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# store best results\nbest_epoch = np.argmin(history.history['val_loss'])\nbest_loss = history.history['val_loss'][best_epoch]\nbest_acc_bowel = history.history['val_bowel_accuracy'][best_epoch]\nbest_acc_extra = history.history['val_extra_accuracy'][best_epoch]\nbest_acc_liver = history.history['val_liver_accuracy'][best_epoch]\nbest_acc_kidney = history.history['val_kidney_accuracy'][best_epoch]\nbest_acc_spleen = history.history['val_spleen_accuracy'][best_epoch]\n\n# Find mean accuracy\nbest_acc = np.mean(\n    [best_acc_bowel,\n     best_acc_extra,\n     best_acc_liver,\n     best_acc_kidney,\n     best_acc_spleen\n])\n\n\nprint(f'>>>> BEST Loss  : {best_loss:.3f}\\n>>>> BEST Acc   : {best_acc:.3f}\\n>>>> BEST Epoch : {best_epoch}\\n')\nprint('ORGAN Acc:')\nprint(f'  >>>> {\"Bowel\".ljust(15)} : {best_acc_bowel:.3f}')\nprint(f'  >>>> {\"Extravasation\".ljust(15)} : {best_acc_extra:.3f}')\nprint(f'  >>>> {\"Liver\".ljust(15)} : {best_acc_liver:.3f}')\nprint(f'  >>>> {\"Kidney\".ljust(15)} : {best_acc_kidney:.3f}')\nprint(f'  >>>> {\"Spleen\".ljust(15)} : {best_acc_spleen:.3f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Store the model for inference**","metadata":{}},{"cell_type":"code","source":"# Save the model\nmodel.save(\"rsna-atd_model2.keras\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\n# Load the saved model\nloaded_model = load_model(\"rsna-atd_model2.keras\")\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model 3**","metadata":{}},{"cell_type":"code","source":"\nmodel3_df = pd.merge(labels,train, on='patient_id', how='inner')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Splitting datasets into groups**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\n# Column definitions\ny_train_columns = [\"bowel_injury\", \"extravasation_injury\",\n                   \"kidney_healthy\", \"kidney_low\", \"kidney_high\",\n                   \"liver_healthy\", \"liver_low\", \"liver_high\",\n                   \"spleen_healthy\", \"spleen_low\", \"spleen_high\"]\n\n# Function to handle the split for each group\ndef split_group(group, test_size=0.2):\n    if len(group) == 1:\n        return (group, pd.DataFrame()) if np.random.rand() < test_size else (pd.DataFrame(), group)\n    else:\n        return train_test_split(group, test_size=test_size, random_state=42)\n\n# Initialize the train and validation datasets\ntrain_df = pd.DataFrame()\nval_df = pd.DataFrame()\n\n# Iterate through the groups and split them, handling single-sample groups\nfor _, group in model3_df.groupby(y_train_columns):\n    train_group, val_group = split_group(group)\n    train_df = pd.concat([train_df, train_group], ignore_index=True)  # concatenate with train_df\n    val_df = pd.concat([val_df, val_group], ignore_index=True)       # concatenate with val_df\n\n# Create generators for training and validation\nbatch_size = 32\ntrain_gen = batch_generator(train_df, batch_size=batch_size)\nval_gen = batch_generator(val_df, batch_size=batch_size)\n\n# Number of steps per epoch\ntrain_steps_per_epoch = len(train_df) // batch_size\nval_steps_per_epoch = len(val_df) // batch_size\n\n# Print shapes to verify\nprint(train_df.shape, val_df.shape)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model3_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Checking data leakage**","metadata":{}},{"cell_type":"code","source":"# Get unique patient_ids for both train and validation datasets\ntrain_patient_ids = set(train_df['patient_id'].unique())\nval_patient_ids = set(val_df['patient_id'].unique())\n\n# Check the intersection of patient_ids\ncommon_patient_ids = train_patient_ids.intersection(val_patient_ids)\n\nif len(common_patient_ids) == 0:\n    print(\"No patient_id is shared between training and validation datasets.\")\nelse:\n    print(f\"There are {len(common_patient_ids)} patient_ids shared between training and validation datasets.\")\n    print(\"Shared patient_ids:\", common_patient_ids)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Checking for class balance**","metadata":{}},{"cell_type":"code","source":"# Checking class balance for each individual label in the training set\ntrain_class_balance_readable = {}\n\nfor column in y_train_columns:\n    train_class_balance_readable[column] = train_df[column].value_counts().to_dict()\n\nprint(\"Class balance in training set:\")\nprint(train_class_balance_readable)\n\n# Checking class balance for each individual label in the validation set\nval_class_balance_readable = {}\n\nfor column in y_train_columns:\n    val_class_balance_readable[column] = val_df[column].value_counts().to_dict()\n\nprint(\"\\nClass balance in validation set:\")\nprint(val_class_balance_readable)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**IMage augmentation and preprosssing**","metadata":{}},{"cell_type":"markdown","source":"import pydicom\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport gc\n\n# Read DICOM image from a file path\ndef read_dicom_image(path):\n    dicom_img = pydicom.dcmread(path)\n    return dicom_img.pixel_array\n\n# Augmentation\ndef augment_image(img):\n    # Random rotation\n    angle = np.random.randint(-15, 15)\n    M = cv2.getRotationMatrix2D((img.shape[1] // 2, img.shape[0] // 2), angle, 1)\n    img = cv2.warpAffine(img, M, (img.shape[1], img.shape[0]))\n\n    # Random flip\n    if np.random.rand() > 0.5:\n        img = cv2.flip(img, 1)  # horizontal flip\n\n    return img\n\n# Process the image\ndef process_image(img):\n    rescaled_img = cv2.normalize(img, None, 0, 1, cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n    equalized_img = cv2.equalizeHist((rescaled_img * 255).astype(np.uint8))\n    k_size = (5, 5)\n    sigma = 0.5\n    smoothed_img = cv2.GaussianBlur(equalized_img, k_size, sigma)\n    padding_size = (20, 20, 20, 20)\n    padded_img = np.pad(smoothed_img, ((padding_size[0], padding_size[1]), (padding_size[2], padding_size[3])), mode='constant', constant_values=0)\n    \n    # Resize to a fixed size\n    resized_img = cv2.resize(padded_img, (256, 256))\n    \n    return resized_img / 255.0  # normalize to [0,1]\n\n# Process a batch of images\ndef process_batch(batch):\n    batch_images = []\n    for index, row in batch.iterrows():\n        img = read_dicom_image(row['img_path'])\n        processed_img = process_image(img)\n        batch_images.append(processed_img)\n    return np.stack(batch_images)\n\ndef image_generator(labels_df, batch_size):\n    target_labels = [\"bowel_injury\", \"extravasation_injury\", \"kidney_healthy\", \"kidney_low\", \"kidney_high\", \"liver_healthy\", \"liver_low\", \"liver_high\", \"spleen_healthy\", \"spleen_low\", \"spleen_high\"]\n    num_features = len(target_labels)\n    num_samples = len(labels_df)\n    \n    while True:\n        for start in range(0, num_samples, batch_size):\n            end = min(start + batch_size, num_samples)\n            batch = labels_df.iloc[start:end]\n            batch_images = process_batch(batch)\n            \n           # Group labels into categories\n            labels_sliced = tf.cast(batch[target_labels].values, tf.float32)\n            \n            # Modify the labels for kidney, liver, and spleen categories\n            labels = (\n                labels_sliced[:, 0:1],  # bowel\n                labels_sliced[:, 1:2],  # extravasation\n                labels_sliced[:, 2:5],  # kidney (3 units)\n                labels_sliced[:, 5:8],  # liver (3 units)\n                labels_sliced[:, 8:11]  # spleen (3 units)\n            )\n            \n            \n            yield batch_images, labels\n            \n            # Free up memory\n            del batch_images\n            gc.collect()\n\n# Sample usage\n#batch_size = 100\ntrain_gen = image_generator(train_df, batch_size=batch_size)\nval_gen = image_generator(val_df, batch_size=batch_size)\n\n# To get the next batch of images and labels for training:\nX_train_batch, y_train_batch = next(train_gen)\n\n# And for validation:\nX_val_batch, y_val_batch = next(val_gen)","metadata":{}},{"cell_type":"code","source":"import pydicom\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport gc\n\n# Read DICOM image from a file path\ndef read_dicom_image(path):\n    dicom_img = pydicom.dcmread(path)\n    return dicom_img.pixel_array\n\n# Augmentation\ndef augment_image(img):\n    # Random rotation\n    angle = np.random.randint(-15, 15)\n    M = cv2.getRotationMatrix2D((img.shape[1] // 2, img.shape[0] // 2), angle, 1)\n    img = cv2.warpAffine(img, M, (img.shape[1], img.shape[0]))\n\n    # Random flip\n    if np.random.rand() > 0.5:\n        img = cv2.flip(img, 1)  # horizontal flip\n\n    return img\n\n# Process the image\ndef process_image(img):\n    rescaled_img = cv2.normalize(img, None, 0, 1, cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n    equalized_img = cv2.equalizeHist((rescaled_img * 255).astype(np.uint8))\n    k_size = (5, 5)\n    sigma = 0.5\n    smoothed_img = cv2.GaussianBlur(equalized_img, k_size, sigma)\n    padding_size = (20, 20, 20, 20)\n    padded_img = np.pad(smoothed_img, ((padding_size[0], padding_size[1]), (padding_size[2], padding_size[3])), mode='constant', constant_values=0)\n    \n    # Resize to a fixed size\n    resized_img = cv2.resize(padded_img, (256, 256))\n    \n    return resized_img / 255.0  # normalize to [0,1]\n\n# Process a batch of images\ndef process_batch(batch):\n    batch_images = []\n    for index, row in batch.iterrows():\n        img = read_dicom_image(row['img_path'])\n        processed_img = process_image(img)\n        batch_images.append(processed_img)\n    return np.stack(batch_images)\n\ndef image_generator(labels_df, batch_size):\n    num_samples = len(labels_df)\n    \n    while True:\n        for start in range(0, num_samples, batch_size):\n            end = min(start + batch_size, num_samples)\n            batch = labels_df.iloc[start:end]\n            batch_images = process_batch(batch)\n            \n            # Define labels for each task\n            labels_bowel = batch['bowel_injury'].values  # Binary classification\n            labels_extravasation = batch['extravasation_injury'].values  # Binary classification\n            \n            labels_kidney = batch[['kidney_healthy', 'kidney_low', 'kidney_high']].values  # Multi-class (3 classes)\n            labels_liver = batch[['liver_healthy', 'liver_low', 'liver_high']].values  # Multi-class (3 classes)\n            labels_spleen = batch[['spleen_healthy', 'spleen_low', 'spleen_high']].values  # Multi-class (3 classes)\n            \n            # Yield the batch of images and labels\n            yield batch_images, [labels_bowel, labels_extravasation, labels_kidney, labels_liver, labels_spleen]\n            \n            # Free up memory\n            del batch_images\n            gc.collect()\n\n# Sample usage\n#batch_size = 100\ntrain_gen = image_generator(train_df, batch_size=batch_size)\nval_gen = image_generator(val_df, batch_size=batch_size)\n\n# To get the next batch of images and labels for training:\nX_train_batch, y_train_batch = next(train_gen)\n\n# And for validation:\nX_val_batch, y_val_batch = next(val_gen)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\n\n# Mocking some values for config\nclass Config:\n    IMAGE_SIZE = (256, 256)\n    BATCH_SIZE = 32\n\nconfig = Config()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMAGE_SIZE = (256, 256)\n\n# 3. Define a function to create the model within the strategy scope\ndef create_model():\n    inputs = tf.keras.Input(shape=IMAGE_SIZE + (1,), batch_size=32)\n\n    # Define Backbone\n    backbone = tf.keras.applications.ResNet50(include_top=False, weights=None, input_tensor=inputs)\n    x = backbone.output\n\n    # GAP to get the activation maps\n    gap = tf.keras.layers.GlobalAveragePooling2D()\n    x = gap(x)\n\n    # Adding dropout for regularization\n    x = tf.keras.layers.Dropout(0.5)(x)\n\n    # Define 'necks' for each head\n    x_bowel = tf.keras.layers.Dense(32, activation='silu')(x)\n    x_extra = tf.keras.layers.Dense(32, activation='silu')(x)\n    x_liver = tf.keras.layers.Dense(32, activation='silu')(x)\n    x_kidney = tf.keras.layers.Dense(32, activation='silu')(x)\n    x_spleen = tf.keras.layers.Dense(32, activation='silu')(x)\n\n    # Define heads\n    out_bowel = tf.keras.layers.Dense(1, name='bowel', activation='sigmoid')(x_bowel)\n    out_extra = tf.keras.layers.Dense(1, name='extra', activation='sigmoid')(x_extra)\n    out_liver = tf.keras.layers.Dense(3, name='liver', activation='softmax')(x_liver)\n    out_kidney = tf.keras.layers.Dense(3, name='kidney', activation='softmax')(x_kidney)\n    out_spleen = tf.keras.layers.Dense(3, name='spleen', activation='softmax')(x_spleen)\n\n    # Concatenate the outputs\n    outputs = [out_bowel, out_extra, out_liver, out_kidney, out_spleen]\n\n    # Create model\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    return model\n\n# 4. Create the model within the strategy scope\nwith tpu_strategy.scope():\n    model = create_model()\n    \n    # Cosine Decay\n    cosine_decay = tf.keras.optimizers.schedules.CosineDecay(\n        initial_learning_rate=1e-4,\n        decay_steps=decay_steps,\n        alpha=0.0\n    )\n\n    # Compile the model\n    optimizer = tf.keras.optimizers.Adam(learning_rate=cosine_decay)\n    loss = {\n        \"bowel\": tf.keras.losses.BinaryCrossentropy(),\n        \"extra\": tf.keras.losses.BinaryCrossentropy(),\n        \"liver\": tf.keras.losses.CategoricalCrossentropy(),\n        \"kidney\": tf.keras.losses.CategoricalCrossentropy(),\n        \"spleen\": tf.keras.losses.CategoricalCrossentropy(),\n    }\n    metrics = {\n        \"bowel\": [\"accuracy\"],\n        \"extra\": [\"accuracy\"],\n        \"liver\": [\"accuracy\"],\n        \"kidney\": [\"accuracy\"],\n        \"spleen\": [\"accuracy\"],\n    }\n\n    \n    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n\nmodel.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mocking some values needed for training\n# These are just placeholders since we don't have the actual values in this environment.\nwarmup_steps = 10\ndecay_steps = 100\nbatch_size=64\n# 1. Set Parameters\nepochs = 10\nsteps_per_epoch = len(train_df) // batch_size\nvalidation_steps = len(val_df) // batch_size\n\n# 2. (Optional) Callbacks\n# For demonstration, we'll use the ModelCheckpoint callback to save the best model.\ncheckpoint_callback = keras.callbacks.ModelCheckpoint(\n    'best_model.h6', \n    monitor='val_loss', \n    verbose=1, \n    save_best_only=True, \n    mode='min'\n)\n\n# 3. Train the Model\n\n# The code in triple quotes is what you'd execute in your environment.\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nhistory = model.fit(\n    train_gen,\n    steps_per_epoch=steps_per_epoch,\n    epochs=epochs,\n    validation_data=val_gen,\n    validation_steps=validation_steps,\n    callbacks=[checkpoint_callback]\n)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Visualize the training plots**","metadata":{}},{"cell_type":"code","source":"# Create a 3x2 grid for the subplots\nfig, axes = plt.subplots(5, 1, figsize=(5, 15))\n\n# Flatten axes to iterate through them\naxes = axes.flatten()\n\n# Iterate through the metrics and plot them\nfor i, name in enumerate([\"bowel\", \"extra\", \"kidney\", \"liver\", \"spleen\"]):\n    # Plot training accuracy\n    axes[i].plot(history.history[name + '_accuracy'], label='Training ' + name)\n    # Plot validation accuracy\n    axes[i].plot(history.history['val_' + name + '_accuracy'], label='Validation ' + name)\n    axes[i].set_title(name)\n    axes[i].set_xlabel('Epoch')\n    axes[i].set_ylabel('Accuracy')\n    axes[i].legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history[\"loss\"], label=\"loss\")\nplt.plot(history.history[\"val_loss\"], label=\"val loss\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# store best results\nbest_epoch = np.argmin(history.history['val_loss'])\nbest_loss = history.history['val_loss'][best_epoch]\nbest_acc_bowel = history.history['val_bowel_accuracy'][best_epoch]\nbest_acc_extra = history.history['val_extra_accuracy'][best_epoch]\nbest_acc_liver = history.history['val_liver_accuracy'][best_epoch]\nbest_acc_kidney = history.history['val_kidney_accuracy'][best_epoch]\nbest_acc_spleen = history.history['val_spleen_accuracy'][best_epoch]\n\n# Find mean accuracy\nbest_acc = np.mean(\n    [best_acc_bowel,\n     best_acc_extra,\n     best_acc_liver,\n     best_acc_kidney,\n     best_acc_spleen\n])\n\n\nprint(f'>>>> BEST Loss  : {best_loss:.3f}\\n>>>> BEST Acc   : {best_acc:.3f}\\n>>>> BEST Epoch : {best_epoch}\\n')\nprint('ORGAN Acc:')\nprint(f'  >>>> {\"Bowel\".ljust(15)} : {best_acc_bowel:.3f}')\nprint(f'  >>>> {\"Extravasation\".ljust(15)} : {best_acc_extra:.3f}')\nprint(f'  >>>> {\"Liver\".ljust(15)} : {best_acc_liver:.3f}')\nprint(f'  >>>> {\"Kidney\".ljust(15)} : {best_acc_kidney:.3f}')\nprint(f'  >>>> {\"Spleen\".ljust(15)} : {best_acc_spleen:.3f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"**Store the model for inference**","metadata":{}},{"cell_type":"code","source":"# Save the model\nmodel.save(\"rsna-atd_model3.keras\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\n# Load the saved model\nloaded_model = load_model(\"rsna-atd_model3.keras\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"**Models 4**","metadata":{}},{"cell_type":"code","source":"!pip install tensorflow==2.x\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install --upgrade pip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install keras-tuner\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import kerastuner as kt\n\n\n# Define a function to build the model\ndef build_model(hp):\n    inputs = keras.Input(shape=config.IMAGE_SIZE + (1,), batch_size=64)\n    backbone = keras.applications.ResNet50(include_top=False, weights=None, input_tensor=inputs)\n    x = backbone.output\n    gap = keras.layers.GlobalAveragePooling2D()(x)\n    \n    # Hyperparameters to tune\n    learning_rate = hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='log')\n    dropout_rate = hp.Float('dropout_rate', min_value=0.2, max_value=0.7, step=0.1)\n    weight_decay = hp.Float('weight_decay', min_value=1e-5, max_value=1e-2, sampling='log')\n    \n    x = keras.layers.Dropout(dropout_rate)(gap)\n    \n    # Define 'necks' for each head with 64 units\n    units = 64\n    x_bowel = keras.layers.Dense(units, activation='silu', kernel_regularizer=keras.regularizers.l2(weight_decay))(x)\n    x_extra = keras.layers.Dense(units, activation='silu', kernel_regularizer=keras.regularizers.l2(weight_decay))(x)\n    x_liver = keras.layers.Dense(units, activation='silu', kernel_regularizer=keras.regularizers.l2(weight_decay))(x)\n    x_kidney = keras.layers.Dense(units, activation='silu', kernel_regularizer=keras.regularizers.l2(weight_decay))(x)\n    x_spleen = keras.layers.Dense(units, activation='silu', kernel_regularizer=keras.regularizers.l2(weight_decay))(x)\n    \n    # Define heads for each target label\n    out_bowel = keras.layers.Dense(1, name='bowel', activation='sigmoid')(x_bowel)\n    out_extra = keras.layers.Dense(1, name='extra', activation='sigmoid')(x_extra)\n    out_liver = keras.layers.Dense(3, name='liver', activation='softmax')(x_liver)\n    out_kidney = keras.layers.Dense(3, name='kidney', activation='softmax')(x_kidney)\n    out_spleen = keras.layers.Dense(3, name='spleen', activation='softmax')(x_spleen)\n    \n    model = keras.Model(inputs=inputs, outputs=[out_bowel, out_extra, out_liver, out_kidney, out_spleen])\n    \n    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n    loss = {\n        \"bowel\": keras.losses.BinaryCrossentropy(),\n        \"extra\": keras.losses.BinaryCrossentropy(),\n        \"liver\": keras.losses.CategoricalCrossentropy(),\n        \"kidney\": keras.losses.CategoricalCrossentropy(),\n        \"spleen\": keras.losses.CategoricalCrossentropy(),\n    }\n    \n    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n    \n    return model\n\nwith tpu_strategy.scope():\n    # Create a tuner\n    tuner = kt.Hyperband(\n        build_model,\n        objective='val_loss',\n        max_epochs=10,  # Maximum number of training epochs\n        factor=3,        # Reduction factor for the number of models\n        directory='hyperparameter_tuning',\n        project_name='my_image_classification'\n    )\n\n    # Define callbacks for early stopping\n    early_stopping = keras.callbacks.EarlyStopping(\n        monitor='val_loss',\n        patience=5,\n        restore_best_weights=True\n    )\n\n    # Perform hyperparameter tuning\n    tuner.search(\n        train_gen,\n        steps_per_epoch=train_steps_per_epoch,\n        validation_data=val_gen,\n        validation_steps=val_steps_per_epoch,\n        epochs=10,\n        callbacks=[early_stopping],\n    )\n\n    # Get the best hyperparameters\n    best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n\n    # Train the final model with the best hyperparameters\n    best_model = tuner.hypermodel.build(best_hyperparameters)\n    best_model.summary()\n\n    history = best_model.fit(\n        train_gen,\n        steps_per_epoch=train_steps_per_epoch,\n        validation_data=val_gen,\n        validation_steps=val_steps_per_epoch,\n        epochs=50,  # You can adjust the number of epochs\n        callbacks=[early_stopping],\n    )\n\n# Evaluate the final model\ntest_loss, test_accuracy = best_model.evaluate(test_gen, steps=test_steps)\nprint(f\"Test Loss: {test_loss:.4f}\")\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a 3x2 grid for the subplots\nfig, axes = plt.subplots(5, 1, figsize=(5, 15))\n\n# Flatten axes to iterate through them\naxes = axes.flatten()\n\n# Iterate through the metrics and plot them\nfor i, name in enumerate([\"bowel\", \"extra\", \"kidney\", \"liver\", \"spleen\"]):\n    # Plot training accuracy\n    axes[i].plot(history.history[name + '_accuracy'], label='Training ' + name)\n    # Plot validation accuracy\n    axes[i].plot(history.history['val_' + name + '_accuracy'], label='Validation ' + name)\n    axes[i].set_title(name)\n    axes[i].set_xlabel('Epoch')\n    axes[i].set_ylabel('Accuracy')\n    axes[i].legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history[\"loss\"], label=\"loss\")\nplt.plot(history.history[\"val_loss\"], label=\"val loss\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import kerastuner as kt\nimport tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nimport pydicom\nimport cv2\nimport pandas as pd\nimport gc\n\n# Define functions for reading DICOM images, augmentation, and image preprocessing (as provided earlier)\n\n# Define your image generator and data loading (as provided earlier)\n\n# Define a function to build the model\ndef build_model(hp):\n    inputs = keras.Input(shape=config.IMAGE_SIZE + (1,), batch_size=64)\n    backbone = keras.applications.DenseNet121(include_top=False, weights=None, input_tensor=inputs)\n    x = backbone.output\n    gap = keras.layers.GlobalAveragePooling2D()(x)\n    \n    # Hyperparameters to tune\n    learning_rate = hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='log')\n    dropout_rate = hp.Float('dropout_rate', min_value=0.2, max_value=0.7, step=0.1)\n    weight_decay = hp.Float('weight_decay', min_value=1e-5, max_value=1e-2, sampling='log')\n    \n    x = keras.layers.Dropout(dropout_rate)(gap)\n    \n    # Define 'necks' for each head with 64 units\n    units = 32\n    x_bowel = keras.layers.Dense(units, activation='silu', kernel_regularizer=keras.regularizers.l2(weight_decay))(x)\n    x_extra = keras.layers.Dense(units, activation='silu', kernel_regularizer=keras.regularizers.l2(weight_decay))(x)\n    x_liver = keras.layers.Dense(units, activation='silu', kernel_regularizer=keras.regularizers.l2(weight_decay))(x)\n    x_kidney = keras.layers.Dense(units, activation='silu', kernel_regularizer=keras.regularizers.l2(weight_decay))(x)\n    x_spleen = keras.layers.Dense(units, activation='silu', kernel_regularizer=keras.regularizers.l2(weight_decay))(x)\n    \n    # Define heads for each target label\n    out_bowel = keras.layers.Dense(1, name='bowel', activation='sigmoid')(x_bowel)\n    out_extra = keras.layers.Dense(1, name='extra', activation='sigmoid')(x_extra)\n    out_liver = keras.layers.Dense(3, name='liver', activation='softmax')(x_liver)\n    out_kidney = keras.layers.Dense(3, name='kidney', activation='softmax')(x_kidney)\n    out_spleen = keras.layers.Dense(3, name='spleen', activation='softmax')(x_spleen)\n    \n    model = keras.Model(inputs=inputs, outputs=[out_bowel, out_extra, out_liver, out_kidney, out_spleen])\n    \n    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n    loss = {\n        \"bowel\": keras.losses.BinaryCrossentropy(),\n        \"extra\": keras.losses.BinaryCrossentropy(),\n        \"liver\": keras.losses.CategoricalCrossentropy(),\n        \"kidney\": keras.losses.CategoricalCrossentropy(),\n        \"spleen\": keras.losses.CategoricalCrossentropy(),\n    }\n    \n    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n    \n    return model\n\nwith tpu_strategy.scope():\n    # Create a tuner\n    tuner = kt.Hyperband(\n        build_model,\n        objective='val_loss',\n        max_epochs=10,  # Maximum number of training epochs\n        factor=3,        # Reduction factor for the number of models\n        directory='hyperparameter_tuning',\n        project_name='my_image_classification'\n    )\n\n    # Define callbacks for early stopping\n    early_stopping = keras.callbacks.EarlyStopping(\n        monitor='val_loss',\n        patience=5,\n        restore_best_weights=True\n    )\n\n    # Perform hyperparameter tuning\n    tuner.search(\n        train_gen,\n        steps_per_epoch=train_steps_per_epoch,\n        validation_data=val_gen,\n        validation_steps=val_steps_per_epoch,\n        epochs=10,\n        callbacks=[early_stopping],\n    )\n\n    # Get the best hyperparameters\n    best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n\n    # Train the final model with the best hyperparameters\n    best_model = tuner.hypermodel.build(best_hyperparameters)\n    best_model.summary()\n\n    history = best_model.fit(\n        train_gen,\n        steps_per_epoch=train_steps_per_epoch,\n        validation_data=val_gen,\n        validation_steps=val_steps_per_epoch,\n        epochs=50,  # You can adjust the number of epochs\n        callbacks=[early_stopping],\n    )\n\n# Evaluate the final model\ntest_loss, test_accuracy = best_model.evaluate(test_gen, steps=test_steps)\nprint(f\"Test Loss: {test_loss:.4f}\")\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}